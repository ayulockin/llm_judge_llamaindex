{"question":"Hey I have a question about using wandb with fastapi in a prod environment. is it recommended to initialize wandb within a specific route function, ie\n\n`@app.route('\/')\ndef my_function():\n    wandb.init(...)\n`\nor should i initialize beforehand:\n\n`wandb.init(...)\n@app.route('\/')\ndef my_function():\n    ...`\n\nI'm getting a long list of log items in the console and many of them are empty.","answer":"When integrating `wandb` (Weights & Biases) with a FastAPI application, it's important to consider the nature of your application and how you're using `wandb`. FastAPI is an asynchronous web framework, and initializing `wandb` within a route function could lead to multiple initializations if the route is hit multiple times, which is not ideal.\n\nHere are some guidelines to help you decide where to place the `wandb.init()` call:\n\n1. **One-time Initialization**: If you need to track metrics across the entire application lifecycle and not just within a specific route, you should initialize `wandb` outside of the route function, preferably at the start of your application. This way, `wandb` is initialized once when your FastAPI app starts.\n\n2. **Per-Request Initialization**: If you need to track metrics for a specific route and each request is considered a separate experiment or run, you might initialize `wandb` within the route function. However, this is less common and could lead to a cluttered dashboard with many runs, which might be what you're experiencing with the empty log items.\n\n3. **Background Tasks**: If you're running long-running tasks or jobs that are kicked off from a FastAPI route, you might want to use FastAPI's background tasks to initialize and log to `wandb`. This way, the `wandb` run is associated with the background task and not the web request itself.\n\nHere's an example of how you might structure it for a one-time initialization:\n\n```python\nimport wandb\nfrom fastapi import FastAPI\n\n# Initialize wandb once when the application starts\nwandb.init(project=\"my_project\", entity=\"my_entity\")\n\napp = FastAPI()\n\n@app.get(\"\/\")\nasync def my_function():\n    # Log something to wandb\n    wandb.log({\"metric\": value})\n    return {\"Hello\": \"World\"}\n```\n\nFor per-request initialization (less common):\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"\/\")\nasync def my_function():\n    # Initialize wandb for this specific request\/run\n    run = wandb.init(project=\"my_project\", entity=\"my_entity\", reinit=True)\n    # Log something to wandb\n    wandb.log({\"metric\": value})\n    # Finish the run when done logging\n    run.finish()\n    return {\"Hello\": \"World\"}\n```\n\nRemember to finish the run with `run.finish()` if you're initializing `wandb` within a route function to ensure that resources are cleaned up properly.\n\nIf you're seeing a lot of empty log items, it could be due to initializing `wandb` multiple times without logging any metrics or due to incorrect usage within an asynchronous environment. Make sure that you're logging the relevant metrics after each initialization and that you're managing the lifecycle of your `wandb` runs correctly.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/track\/tracking-faq\n\n`InitStartError: Error communicating with wandb process`\n\n\nThis error indicates that the library is having difficulty launching the process which synchronizes data to the server.\n\n\nThe following workarounds can help resolve the issue in certain environments:\n\n\n\n\n```\nwandb.init(settings=wandb.Settings(start_method=\"fork\"))\n\n```\n\n\nFor versions prior to `0.13.0` we suggest using:\n\n\n\n```\nwandb.init(settings=wandb.Settings(start_method=\"thread\"))\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/logging-faqs\n\nWhat happens if I pass a class attribute into wandb.log()?\n\n\nIt is generally not recommended to pass class attributes into `wandb.log()` as the attribute may change before the network call is made. If you are storing metrics as the attribute of a class, it is recommended to deep copy the attribute to ensure the metric logged matches the value of the attribute at the time that `wandb.log()` was called.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/fastai\/\n\nLog with W&B\n\n\n**a)** Sign up for a free account at <https:\/\/wandb.ai\/site> and then log in to your wandb account.\n\n\n**b)** Install the wandb library on your machine in a Python 3 environment using `pip`\n\n\n**c)** log in to the wandb library on your machine. You will find your API key here: <https:\/\/wandb.ai\/authorize>.\n\n\n\n\n```\npip install wandb\nwandb login\n\n```\n\n\n\n```\n!pip install wandb\n\nimport wandb\nwandb.login()\n\n```\n\n\nThen add the `WandbCallback` to the `learner` or `fit` method:\n\n\n\n```\nimport wandb\nfrom fastai.callback.wandb import *\n\n# start logging a wandb run\nwandb.init(project=\"my_project\")\n\n# To log only during one training phase\nlearn.fit(..., cbs=WandbCallback())\n\n# To log continuously for all training phases\nlearn = learner(..., cbs=WandbCallback())\n\n```\n\n:::info  \n\nIf you use version 1 of Fastai, refer to the Fastai v1 docs.  \n\n:::\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/fastai\/\n\nLogging only on the main process\n\n\nIn the examples above, `wandb` launches one run per process. At the end of the training, you will end up with two runs. This can sometimes be confusing, and you may want to log only on the main process. To do so, you will have to detect in which process you are manually and avoid creating runs (calling `wandb.init` in all other processes)\n\n\n\n\n```\nimport wandb\nfrom fastai.vision.all import *\nfrom fastai.distributed import *\nfrom fastai.callback.wandb import WandbCallback\n\nwandb.require(experiment=\"service\")\npath = rank0_first(lambda: untar_data(URLs.PETS) \/ \"images\")\n\n\ndef train():\n    cb = []\n    dls = ImageDataLoaders.from_name_func(\n        path,\n        get_image_files(path),\n        valid_pct=0.2,\n        label_func=lambda x: x[0].isupper(),\n        item_tfms=Resize(224),\n    )\n    if rank_distrib() == 0:\n        run = wandb.init(\"fastai_ddp\", entity=\"capecape\")\n        cb = WandbCallback()\n    learn = vision_learner(dls, resnet34, metrics=error_rate, cbs=cb).to_fp16()\n    with learn.distrib_ctx(sync_bn=False):\n        learn.fit(1)\n\n\nif __name__ == \"__main__\":\n    train()\n\n```\n\nin your terminal call:\n\n\n\n```\n$ torchrun --nproc_per_node 2 train.py\n\n```\n\n\n\n```\nimport wandb\nfrom fastai.vision.all import *\n\nfrom accelerate import notebook_launcher\nfrom fastai.distributed import *\nfrom fastai.callback.wandb import WandbCallback\n\nwandb.require(experiment=\"service\")\npath = untar_data(URLs.PETS) \/ \"images\"\n\n\ndef train():\n    cb = []\n    dls = ImageDataLoaders.from_name_func(\n        path,\n        get_image_files(path),\n        valid_pct=0.2,\n        label_func=lambda x: x[0].isupper(),\n        item_tfms=Resize(224),\n    )\n    if rank_distrib() == 0:\n        run = wandb.init(\"fastai_ddp\", entity=\"capecape\")\n        cb = WandbCallback()\n    learn = vision_learner(dls, resnet34, metrics=error_rate, cbs=cb).to_fp16()\n    with learn.distrib_ctx(in_notebook=True, sync_bn=False):\n        learn.fit(1)\n\n\nnotebook_launcher(train, num_processes=2)\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/tracking-faq\n\nHow can I use wandb with multiprocessing, e.g. distributed training?\n\n\nIf your training program uses multiple processes you will need to structure your program to avoid making wandb method calls from processes where you did not run `wandb.init()`.\\  \n\n\\  \n\nThere are several approaches to managing multiprocess training:\n\n\n1. Call `wandb.init` in all your processes, using the group keyword argument to define a shared group. Each process will have its own wandb run and the UI will group the training processes together.\n2. Call `wandb.init` from just one process and pass data to be logged over multiprocessing queues.\n\n\n:::info  \n\nCheck out the Distributed Training Guide for more detail on these two approaches, including code examples with Torch DDP.  \n\n:::\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer clearly explains the recommended practices for integrating wandb with a FastAPI application, offering both one-time initialization at the application start and per-request initialization within a route function, if necessary. The answer also mentions the possible reasons for seeing many empty log items in the console, such as multiple initializations or incorrect usage within an asynchronous environment, which directly corresponds to the user's observations.\n"}
{"question":"Hey with wandb is it possible to link from the sweep api?\nbest_run = sweep.best_run()\nI have the best run but it isn't clear to me how to link through the api. the UI works fine but trying to get the api working also. I have been reading the docs but missing something.","answer":"Yes, with `wandb`, you can link to the best run from a sweep using the API. The `best_run()` method of a sweep object will return the best run based on the metric defined in the sweep configuration. Once you have the best run object, you can use its attributes to access various details about the run, including its URL.\n\nHere's an example of how you can retrieve the best run from a sweep and then get the URL for that run:\n\n```python\nimport wandb\n\n# Initialize the API\napi = wandb.Api()\n\n# Replace with your entity, project, and sweep ID\nsweep = api.sweep(\"entity\/project\/sweep_id\")\n\n# Get the best run\nbest_run = sweep.best_run()\n\n# Now you can access the best run's attributes, including the URL\nprint(\"Best run ID:\", best_run.id)\nprint(\"Best run URL:\", best_run.url)\n```\n\nThe `best_run.url` attribute will give you the direct link to the best run in the W&B UI. You can use this URL to navigate to the run's page in your web browser.\n\nIf you're having trouble, make sure that you have the correct permissions and that your API keys are set up correctly. Also, ensure that the `entity`, `project`, and `sweep_id` in the `api.sweep()` call are correctly specified. If you're still facing issues, please provide more details about the error or the problem you're encountering, and I'll be happy to help further.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\n\nGet the best run from a sweep\n\n\nThe following snippet gets the best run from a given sweep.\n\n\n\n```\nimport wandb\n\napi = wandb.Api()\n\nsweep = api.sweep(\"<entity>\/<project>\/<sweep_id>\")\nbest_run = sweep.best_run()\n\n```\n\nThe `best_run` is the run with the best metric as defined by the `metric` parameter in the sweep config.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/ref\/python\/agent\n\nExamples:\n\n\nRun a sample sweep over a function:\n\n\n\n\n```\nimport wandb\n\nsweep_configuration = {\n    \"name\": \"my-awesome-sweep\",\n    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n    \"method\": \"grid\",\n    \"parameters\": {\"a\": {\"values\": [1, 2, 3, 4]}},\n}\n\n\ndef my_train_func():\n    # read the current value of parameter \"a\" from wandb.config\n    wandb.init()\n    a = wandb.config.a\n\n    wandb.log({\"a\": a, \"accuracy\": a + 1})\n\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n# run the sweep\nwandb.agent(sweep_id, function=my_train_func)\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/boosting\/Using_W&B_Sweeps_with_XGBoost.ipynb\n\n2. Initialize the Sweep\n\n\nCalling `wandb.sweep` starts a Sweep Controller \u2013  \n\na centralized process that provides settings of the `parameters` to any who query it  \n\nand expects them to return performance on `metrics` via `wandb` logging.\n\n\n\n```\nsweep_id = wandb.sweep(sweep_config, project=\"XGBoost-sweeps\")\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\n\nGet runs from a specific sweep\n\n\nThis snippet downloads all the runs associated with a particular sweep.\n\n\n\n```\nimport wandb\n\napi = wandb.Api()\n\nsweep = api.sweep(\"<entity>\/<project>\/<sweep_id>\")\nsweep_runs = sweep.runs\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs\n\nHow do I find an artifact from the best run in a sweep?\n\n\nYou can use the following code to retrieve the artifacts associated with the best performing run in a sweep:\n\n\n\n```\napi = wandb.Api()\nsweep = api.sweep(\"entity\/project\/sweep_id\")\nruns = sorted(sweep.runs, key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\nbest_run = runs[0]\nfor artifact in best_run.logged_artifacts():\n    artifact_path = artifact.download()\n    print(artifact_path)\n\n```\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"This answer correctly explains how to use the W&B API to get the best run from a sweep and access its URL."}
{"question":"Explain how I can version datasets with Weights & Biases. How do I create a new version of a dataset? How does Weights & Biases work with versions? What are some best practices for versioning datasets?","answer":"Versioning datasets with Weights & Biases (W&B) is an essential practice for maintaining reproducibility and traceability in machine learning workflows. W&B provides a feature called Artifacts, which allows you to version datasets, models, and other types of files. Here's how you can create a new version of a dataset and work with versions in W&B:\n\n### Creating a New Version of a Dataset with W&B:\n\n1. **Install and Log In**:\n   Make sure you have W&B installed (`pip install wandb`) and you are logged in using `wandb.login()`.\n\n2. **Initialize a W&B Run**:\n   Start a new run to track the process of creating and logging the dataset version.\n   ```python\n   run = wandb.init(project='my_project', job_type='create_dataset')\n   ```\n\n3. **Create or Load Your Dataset**:\n   Prepare your dataset as you normally would, either by creating it from scratch or loading it from a file or database.\n\n4. **Log the Dataset as an Artifact**:\n   Create an artifact (representing your dataset) and add your dataset files to it. Then log the artifact to W&B.\n   ```python\n   artifact = wandb.Artifact('my_dataset', type='dataset', description='description of the dataset')\n   artifact.add_dir('path_to_dataset_directory')\n   run.log_artifact(artifact)\n   ```\n\n5. **Finish the Run**:\n   Once the artifact is logged, you can finish the run.\n   ```python\n   run.finish()\n   ```\n\n### Working with Versions in W&B:\n\n- **Version Tracking**: Each time you log a dataset as an artifact with the same name but different contents, W&B automatically creates a new version. These versions are typically labeled with version numbers like `v1`, `v2`, etc.\n\n- **Using Versions**: When you want to use a specific version of a dataset in your experiments, you can reference the artifact by its name and version number.\n  ```python\n  artifact = run.use_artifact('my_dataset:v3')\n  dataset_path = artifact.download()\n  ```\n\n- **Lineage Tracking**: W&B tracks the lineage of each artifact, so you can see which datasets were used to train which models and the outcomes.\n\n### Best Practices for Versioning Datasets:\n\n1. **Descriptive Names and Annotations**: Use clear, descriptive names for your datasets and include annotations or descriptions to detail the contents or changes in each version.\n\n2. **Incremental Changes**: When possible, log only the changes to the dataset rather than the entire dataset each time. This saves storage and makes it easier to track modifications.\n\n3. **Consistent Naming Conventions**: Use consistent naming conventions for your datasets and versions to make it easier to manage and reference them.\n\n4. **Documentation**: Document the reasons for changes in each version, including what was added, removed, or modified. This can be done in the artifact's description or in a separate changelog.\n\n5. **Immutable Datasets**: Treat dataset versions as immutable. Once a version is logged, it should not be altered. Any changes should result in a new version.\n\n6. **Reference Artifacts in Code**: When using datasets in your training scripts, reference the dataset artifact by name and version to ensure reproducibility.\n\nBy following these practices, you can effectively version your datasets with Weights & Biases, ensuring that your machine learning experiments are reproducible and that you have a clear record of how your datasets evolve over time.","context":"Source:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/intro\/3_in_1_Intro_to_Weights_&_Biases_CV,_NLP_and_RL.ipynb\n\nA 3-in-1 Intro to Weights & Biases: Computer Vision, Natural Language Processing and Reinforcement Learning\n\n\nWeights & Biases is a developer toolkit for machine learning experiment tracking, dataset and model versioning, and collaboration\n\n\n\nIn this mega 3-in-1 notebook you\u2019ll see how Weights and Biases seamlessly integrates into ML code across modalities \u2013 one example each from Computer Vision, NLP and Reinforcement Learning \u2013 as well as across frameworks like PyTorch, Keras, and more.\n\n\nFor the full range of supported integrations, plus more examples, see  \n\nour docs.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/wandb-log\/Configs_in_W&B.ipynb\n\nQuickstart\n\n\nUse Weights & Biases  \n\nfor machine learning experiment tracking, dataset versioning, and project collaboration.\n\n\n\n\n\n\n\nTo get started, just `pip install` the package and log using `wandb.login()`   \n\nIf this is your first time using `wandb`, you\u2019ll need to sign up. It\u2019s easy!\n\n\n\n```\n!pip install -Uq wandb\n\n```\n\n\n```\nimport wandb\n\n```\n\n\n```\nwandb.login()\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/tutorials\/xgboost\n\nIn this notebook\n\n\nIn this colab we\u2019ll cover how Weights and Biases enables regulated entities to   \n\n- **Track and version** their data ETL pipelines (locally or in cloud services such as S3 and GCS)  \n\n- **Track experiment results** and store trained models   \n\n- **Visually inspect** multiple evaluation metrics   \n\n- **Optimize performance** with hyperparameter sweeps\n\n\n**Track Experiments and Results**\n\n\nWe will track all of the training hyperparameters and output metrics in order to generate an Experiments Dashboard:\n\n\n\n**Run a Hyperparameter Sweep to Find the Best HyperParameters**\n\n\nWeights and Biases also enables you to do hyperparameter sweeps, either with our own Sweeps functionality or with our Ray Tune integration. See our docs for a full guide of how to use more advanced hyperparameter sweeps options.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/stable_baselines3\/Stable_Baselines3_wandb_experiment_tracking.ipynb\n\nIntroduction\n\n\nWeights & Biases (W&B) is a tool for machine learning experiment tracking, dataset versioning, and project collaboration.\n\n\n\n\n\n\n\nIn this notebook, you will learn how to track reinforcement learning experiments using W&B. In particular, W&B helps track your experiment configs, metrics, and videos of the agents playing the game. At the end, you should see a run page like https:\/\/wandb.ai\/wandb\/cartpole\\_test\/runs\/37ppqzxc\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/\n\nWhat is W&B?\n\n\nWeights & Biases (W&B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models. \n\n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your models and data are tracked and versioned in a reliable system of record.\n\n\n\nThis diagram outlines the relationship between W&B products.\n\n\n**W&B Models** is a set of lightweight, interoperable tools for machine learning practitioners training and fine-tuning models.  \n\n- Experiments: Machine learning experiment tracking  \n\n- Model Registry: Manage production models centrally  \n\n- Launch: Scale and automate workloads  \n\n- Sweeps: Hyperparameter tuning and model optimization\n\n\n**W&B Prompts** is for LLM debugging and monitoring, including usage of OpenAI\u2019s GPT API.\n\n\n**W&B Platform** is a core set of powerful building blocks for tracking and visualizing data and models, and communicating results.  \n\n- Artifacts: Version assets and track lineage  \n\n- Tables: Visualize and query tabular data  \n\n- Reports: Document and collaborate on your discoveries\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer correctly summarizes the information from the documents, focusing on how to version datasets with Weights & Biases. It provides a step-by-step guide for creating a new version of a dataset using Artifacts in W&B, describes how W&B works with versions, and outlines best practices for versioning datasets. The answer's content does not deviate from the information provided in the documents and aligns well with the concepts of dataset versioning as outlined in W&B's features."}
{"question":"Hi, can anybody help me with this issue? wandb.sdk.service.service.ServiceStartTimeoutError: Timed out waiting for wandb service to start after 30.0 seconds. Try increasing the timeout with the `_service_wait` setting. I have tried to set 300 seconds, reinstalled wandb as well. but I still get this error","answer":"The `wandb.sdk.service.service.ServiceStartTimeoutError` indicates that the Weights & Biases (W&B) service is taking longer than expected to start. This can be due to a variety of reasons, such as network issues, system resource constraints, or problems with the W&B installation.\n\nHere are some steps you can take to troubleshoot and potentially resolve the issue:\n\n1. **Increase the Service Start Timeout:**\n   You've already tried increasing the timeout to 300 seconds, which is a good first step. Make sure you're setting the `_service_wait` parameter correctly when initializing W&B in your script:\n\n   ```python\n   wandb.init(_service_wait=300)\n   ```\n\n2. **Check Network Connectivity:**\n   Ensure that your machine has a stable internet connection and can reach the W&B servers. Sometimes, firewalls or VPNs can interfere with the connection.\n\n3. **Check System Resources:**\n   Make sure your system has enough resources (CPU, memory) to start the W&B service. If your system is under heavy load, it might cause the service to start slowly.\n\n4. **Reinstall W&B:**\n   You've mentioned that you reinstalled W&B, but it's worth doing it again to ensure a clean installation. Use the following commands:\n\n   ```bash\n   pip uninstall wandb\n   pip install wandb --upgrade\n   ```\n\n5. **Check for Conflicting Processes:**\n   Ensure that there are no other processes that might be conflicting with W&B. You can try restarting your machine to clear out any potential conflicts.\n\n6. **Use W&B Service:**\n   If you're using distributed training, make sure you're using the W&B Service to avoid common issues. For W&B SDK version 0.12.5 and above, use the `wandb.require(\"service\")` method in your script:\n\n   ```python\n   def main():\n       wandb.require(\"service\")\n       # rest-of-your-script-goes-here\n   ```\n\n7. **Check W&B Logs:**\n   Look at the W&B logs for any additional information that might help diagnose the issue. The logs are typically located in the `wandb` directory within your project.\n\n8. **Contact W&B Support:**\n   If none of the above steps resolve the issue, consider reaching out to W&B support for assistance. You can contact them through the W&B community Slack or by creating an issue on the W&B GitHub repository.\n\nRemember to always check that you're using the latest version of the W&B SDK, as updates often include bug fixes and improvements that could resolve your issue.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/distributed-training\n\nW&B SDK 0.12.5 and above\n\n\nModify your Python script to enable W&B Service for W&B SDK version 0.12.5 and above. Use the `wandb.require` method and pass the string `\"service\"` within your main function:\n\n\n\n```\nif __name__ == \"__main__\":\n    main()\n\n\ndef main():\n    wandb.require(\"service\")\n    # rest-of-your-script-goes-here\n\n```\n\nFor optimal experience we do recommend you upgrade to the latest version.\n\n\n**W&B SDK 0.12.4 and below**\n\n\nSet the `WANDB_START_METHOD` environment variable to `\"thread\"` to use multithreading instead if you use a W&B SDK version 0.12.4 and below.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/prompts\/quickstart\n\n1. Import WandbCallbackHandler\n\n\nFirst import the `WandbCallbackHandler` and set it up. You can also pass additional parameters `wandb.init` parameteres such as your W&B Project or Entity.\n\n\nYou will see a W&B run start and be asked for your Weights & Biases **API key**. A W&B run link will be generated, here you\u2019ll be able to view your logged LlamaIndex queries and data once you start logging.\n\n\n\n```\nfrom llama_index import ServiceContext\nfrom llama_index.callbacks import CallbackManager, WandbCallbackHandler\n\n# initialise WandbCallbackHandler and pass any wandb.init args\nwandb_args = {\"project\": \"llamaindex\"}\nwandb_callback = WandbCallbackHandler(run_args=wandb_args)\n\n# pass wandb_callback to the service context\ncallback_manager = CallbackManager([wandb_callback])\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/troubleshoot-sweeps\n\n`Cuda out of memory`\n\n\nRefactor your code to use process-based executions if you see this error message. More specifically, rewrite your code to a Python script. In addition, call the W&B Sweep Agent from the CLI, instead of the W&B Python SDK.\n\n\nAs an example, suppose you rewrite your code to a Python script called `train.py`. Add the name of the training script (`train.py`) to your YAML Sweep configuration file (`config.yaml` in this example):\n\n\n\n```\nprogram: train.py\nmethod: bayes\nmetric:\n  name: validation_loss\n  goal: maximize\nparameters:\n  learning_rate:\n    min: 0.0001\n    max: 0.1\n  optimizer:\n    values: [\"adam\", \"sgd\"]\n\n```\n\nNext, add the following to your `train.py` Python script:\n\n\n\n```\nif _name_ == \"_main_\":\n    train()\n\n```\n\nNavigate to your CLI and initialize a W&B Sweep with wandb sweep:\n\n\n\n```\nwandb sweep config.yaml\n\n```\n\nMake a note of the W&B Sweep ID that is returned. Next, start the Sweep job with `wandb agent` with the CLI instead of the Python SDK (`wandb.agent`). Replace `sweep_ID` in the code snippet below with the Sweep ID that was returned in the previous step:\n\n\n\n```\nwandb agent sweep_ID\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/distributed-training\n\nUse W&B Service to avoid common distributed training issues.\n\n\nThere are two common issues you might encounter when using W&B and distributed training:\n\n\n1. **Hanging at the beginning of training** - A `wandb` process can hang if the `wandb` multiprocessing interferes with the multiprocessing from distributed training.\n2. **Hanging at the end of training** - A training job might hang if the `wandb` process does not know when it needs to exit. Call the `wandb.finish()` API at the end of your Python script to tell W&B that the Run finished. The wandb.finish() API will finish uploading data and will cause W&B to exit.\n\n\nWe recommend using the `wandb service` to improve the reliability of your distributed jobs. Both of the preceding training issues are commonly found in versions of the W&B SDK where wandb service is unavailable.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/distributed-training\n\nEnable W&B Service\n\n\nDepending on your version of the W&B SDK, you might already have W&B Service enabled by default.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"This requires more data to debug and probably a support ticket. The problem is with starting the wandb service, which is poorly documented. \n\nThe answer provides a comprehensive list of troubleshooting steps for dealing with the `ServiceStartTimeoutError` in wandb, which correctly addresses the user's issue. It takes into consideration both documentation and general best practices, such as increasing the timeout, checking network connectivity, system resources, reinstalling wandb, and using W&B Service feature from the latest SDK. It also suggests looking at wandb logs for more details and reaching out to W&B support if needed, which is good additional advice.\n"}
{"question":"what is the difference between artifact.add_file and wandb.save","answer":"`artifact.add_file` and `wandb.save` are both methods used in the context of Weights & Biases (W&B) to save and track files, but they serve different purposes and are used in different scenarios.\n\n`artifact.add_file`:\n- This method is used to add a specific file to a W&B `Artifact`, which is a versioned folder-like structure used to organize and keep track of data, models, and other files.\n- When you use `artifact.add_file`, you are explicitly adding a file to an artifact that you intend to log or save as part of your experiment tracking.\n- Artifacts are designed to handle large files and datasets efficiently, with deduplication and versioning capabilities.\n- The `add_file` method is typically used when you want to create a snapshot of your data or model at a specific point in time and log it to W&B for reproducibility and collaboration.\n\nExample:\n```python\nartifact = wandb.Artifact(\"my_artifact\", type=\"dataset\")\nartifact.add_file(\"my_data.csv\")\nrun.log_artifact(artifact)\n```\n\n`wandb.save`:\n- This method is used to save files that are generated during a W&B run so that they are synced with the W&B server and accessible through the W&B user interface.\n- It is typically used for files like model checkpoints, logs, or any other outputs that you want to be automatically uploaded and associated with a specific run.\n- Unlike artifacts, files saved with `wandb.save` are not versioned in the same way, and this method is more suited for run-specific files rather than datasets or models that you might want to use across multiple runs.\n\nExample:\n```python\nwandb.save(\"model_checkpoint.h5\")\n```\n\nIn summary, `artifact.add_file` is used for adding files to a versioned artifact that can be used across different runs, while `wandb.save` is used for saving run-specific files that you want to sync with the W&B server during a particular run.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/create-a-new-artifact-version\n\n`.add`, `.add_file`, `.add_dir`, and `.add_reference`\nartifact.add_file(\"image1.png\")\nrun.log_artifact(artifact)\n\n```\n\n\n```\n\n  <\/TabItem>\n  <TabItem value=\"outside\">\n\nCreate an artifact version outside of a W&B run:\n\n1. Create a new artifact or retrieve an existing one with `wanb.Artifact`. (Line 1)\n2. Add files to the artifact with `.add_file`. (Line 4)\n3. Save the artifact with `.save`. (Line 5)\n\n```python showLineNumbers\nartifact = wandb.Artifact(\"artifact_name\", \"artifact_type\")\n# Add Files and Assets to the artifact using\n# `.add`, `.add_file`, `.add_dir`, and `.add_reference`\nartifact.add_file(\"image1.png\")\nartifact.save()\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/prompts\/WandB_LLM_QA_bot.ipynb\n\nAnatomy of an artifact\n\n\nThe `Artifact` class will correspond to an entry in the W&B Artifact registry. The artifact has  \n\n\\* a name  \n\n\\* a type  \n\n\\* metadata  \n\n\\* description  \n\n\\* files, directory of files, or references\n\n\nExample usage\n\n\n\n```\nrun = wandb.init(project = \"my-project\")\nartifact = wandb.Artifact(name = \"my_artifact\", type = \"data\")\nartifact.add_file(\"\/path\/to\/my\/file.txt\")\nrun.log_artifact(artifact)\nrun.finish()\n\n```\n\n\n```\nrun = wandb.init(project=project_name, entity=entity, job_type=\"dataset\")\n\nplaylist = Playlist(playlist_url)\nplaylist_video_urls = playlist.video_urls[0:TOTAL_EPISODES]\n\nprint(f\"There are total {len(playlist_video_urls)} videos in the playlist.\")\n\nvideo_data = []\nfor video in tqdm(playlist_video_urls, total=len(playlist_video_urls)):\n    try:\n        curr_video_data = {}\n        yt = retry_access_yt_object(video, max_retries=25, interval_secs=2)\n        curr_video_data[\"title\"] = yt.title\n        curr_video_data[\"url\"] = video\n        curr_video_data[\"duration\"] = yt.length\n        curr_video_data[\"publish_date\"] = yt.publish_date.strftime(\"%Y-%m-%d\")\n        loader = YoutubeLoader.from_youtube_url(video)\n        transcript = loader.load()[0].page_content\n        transcript = \" \".join(transcript.split())\n        curr_video_data[\"transcript\"] = transcript\n        curr_video_data[\"total_words\"] = len(transcript.split())\n        video_data.append(curr_video_data)\n    except Exception as inst:\n        print(type(inst))    # the exception type\n        print(inst.args)     # arguments stored in .args\n        print(inst)\n        print(f\"Failed to scrape {video}\")\n\nprint(f\"Total podcast episodes scraped: {len(video_data)}\")\n\n# save the scraped data to a csv file\ndf = pd.DataFrame(video_data)\ndata_path = root_data_dir \/ \"yt_podcast_transcript.csv\"\ndf.to_csv(data_path, index=False)\n\n# upload the scraped data to wandb\nartifact = wandb.Artifact(\"yt_podcast_transcript\", type=\"dataset\")\nartifact.add_file(data_path)\nrun.log_artifact(artifact)\n\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/wandb-artifacts\/Pipeline_Versioning_with_W&B_Artifacts.ipynb\n\n\ud83d\udc23 `artifact.new_file` and \u270d\ufe0f `run.log_artifact`\n\n\nOnce we\u2019ve made an `Artifact` object, we need to add files to it.\n\n\nYou read that right: *files* with an *s*.  \n\n`Artifact`s are structured like directories,  \n\nwith files and sub-directories.\n\n\n\n> \n> **Rule of \ud83d\udc4d**: whenever it makes sense to do so, split the contents  \n> \n> of an `Artifact` up into multiple files. This will help if it comes time to scale!\n> \n> \n> \n\n\nWe use the `new_file` method  \n\nto simultaneously write the file and attach it to the `Artifact`.  \n\nBelow, we\u2019ll use the `add_file` method,  \n\nwhich separates those two steps.\n\n\nOnce we\u2019ve added all of our files, we need to `log_artifact` to wandb.ai.\n\n\nYou\u2019ll notice some URLs appeared in the output,  \n\nincluding one for the Run page.  \n\nThat\u2019s where you can view the results of the `Run`,  \n\nincluding any `Artifact`s that got logged.\n\n\nWe\u2019ll see some examples that make better use of the other components of the Run page below.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/pages\/run-page\n\nFiles Tab\n\n\n* Save files to sync with the run using `wandb.save()`\n* Keep model checkpoints, validation set examples, and more\n* Use the `diff.patch` to restore the exact version of your code  \n\nView a live example \u2192\n\n\n:::info  \n\nThe W&B Artifacts system adds extra features for handling, versioning, and deduplicating large files like datasets and models. We recommend you use Artifacts for tracking inputs and outputs of runs, rather than `wandb.save`. Check out the Artifacts quickstart here.  \n\n:::\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/create-a-new-artifact-version\n\nCreate a new artifact version from an existing version\n\n\nAdd, modify, or remove a subset of files from a previous artifact version without the need to re-index the files that didn\u2019t change. Adding, modifying, or removing a subset of files from a previous artifact version creates a new artifact version known as an *incremental artifact*.\n\n\n\nHere are some scenarios for each type of incremental change you might encounter:\n\n\n* add: you periodically add a new subset of files to a dataset after collecting a new batch.\n* remove: you discovered several duplicate files and want to remove them from your artifact.\n* update: you corrected annotations for a subset of files and want to replace the old files with the correct ones.\n\n\nYou could create an artifact from scratch to perform the same function as an incremental artifact. However, when you create an artifact from scratch, you will need to have all the contents of your artifact on your local disk. When making an incremental change, you can add, remove, or modify a single file without changing the files from a previous artifact version.\n\n\n:::info  \n\nYou can create an incremental artifact within a single run or with a set of runs (distributed mode).  \n\n:::\n\n\nFollow the procedure below to incrementally change an artifact:\n\n\n1. Obtain the artifact version you want to perform an incremental change on:\n\n\n\n\n```\nsaved_artifact = run.use_artifact(\"my_artifact:latest\")\n\n```\n\n\n\n```\nclient = wandb.Api()\nsaved_artifact = client.artifact(\"my_artifact:latest\")\n\n```\n\n\n2. Create a draft with:\n\n\n\n```\ndraft_artifact = saved_artifact.new_draft()\n\n```\n\n3. Perform any incremental changes you want to see in the next version. You can either add, remove, or modify an existing entry.\n\n\nSelect one of the tabs for an example on how to perform each of these changes:\n\n\n\nAdd a file to an existing artifact version with the `add_file` method:\n\n\n\n```\ndraft_artifact.add_file(\"file_to_add.txt\")\n\n```\n\n:::note  \n\nYou can also add multiple files by adding a directory with the `add_dir` method.  \n\n:::\n\n\n\nRemove a file from an existing artifact version with the `remove` method:\n\n\n\n```\ndraft_artifact.remove(\"file_to_remove.txt\")\n\n```\n\n:::note  \n\nYou can also remove multiple files with the `remove` method by passing in a directory path.  \n\n:::\n\n\n\nModify or replace contents by removing the old contents from the draft and adding the new contents back in:\n\n\n\n```\ndraft_artifact.remove(\"modified_file.txt\")\ndraft_artifact.add_file(\"modified_file.txt\")\n\n```\n\n\n\n4. Lastly, log or save your changes. The following tabs show you how to save your changes inside and outside of a W&B run. Select the tab that is appropriate for your use case:\n\n\n\n\n```\nrun.log_artifact(draft_artifact)\n\n```\n\n\n\n```\ndraft_artifact.save()\n\n```\n\n\nPutting it all together, the code examples above look like: \n\n\n\n\n```\nwith wandb.init(job_type=\"modify dataset\") as run:\n    saved_artifact = run.use_artifact(\n        \"my_artifact:latest\"\n    )  # fetch artifact and input it into your run\n    draft_artifact = saved_artifact.new_draft()  # create a draft version\n\n    # modify a subset of files in the draft version\n    draft_artifact.add_file(\"file_to_add.txt\")\n    draft_artifact.remove(\"dir_to_remove\/\")\n    run.log_artifact(\n        artifact\n    )  # log your changes to create a new version and mark it as output to your run\n\n```\n\n\n\n```\nclient = wandb.Api()\nsaved_artifact = client.artifact(\"my_artifact:latest\")  # load your artifact\ndraft_artifact = saved_artifact.new_draft()  # create a draft version\n\n# modify a subset of files in the draft version\ndraft_artifact.remove(\"deleted_file.txt\")\ndraft_artifact.add_file(\"modified_file.txt\")\ndraft_artifact.save()  # commit changes to the draft\n\n```\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer correctly identifies the distinction between `artifact.add_file` and `wandb.save` in the context of Weights & Biases (W&B) based on the provided document. It explains that `artifact.add_file` is for adding files to a versioned W&B Artifact with versioning and deduplication capabilities, while `wandb.save` is for syncing run-specific files with the W&B server. The answer also provides correct example usage for each method, aligning with the usage and intention described in the document."}
{"question":"Hi I have created an academic team, however I don't have admin rights in my own group and cannot access admin features at all","answer":"If you've created an academic team on W&B (Weights & Biases) but find yourself without admin rights, it's possible that there was an error during the team creation process, or perhaps the admin rights were not correctly assigned to your account. Here's what you can do to resolve the issue:\n\n1. **Contact Support**: Reach out to W&B support directly. They can help you verify your account status and rectify any issues with your team's setup. You can contact them through the support chat on the W&B website or by emailing their support team.\n\n2. **Check with System Admin**: If your academic institution has a W&B Server with system admins, check with them to see if they can grant you admin rights. System admins have the ability to configure system settings, create\/delete teams, and view the activity dashboard, as well as manage team roles.\n\n3. **Review Team Creation Process**: Ensure that you followed the correct process for creating a team. If you missed a step or there was an issue during the process, it might have led to the incorrect assignment of roles.\n\n4. **Verify Email Invitation**: If you received an email invitation to join the team, ensure that you accepted the invitation correctly. Sometimes, roles are assigned through the invitation process.\n\n5. **Check Team Settings**: If you have any level of access to the team settings, review them to see if there's an option to change roles from within the team settings page. This might not be possible without admin rights, but it's worth checking.\n\n6. **Ask a Colleague**: If there's another admin in the team, ask them to check your role and adjust it if necessary.\n\nRemember, only team admins have the ability to add and remove other admins or team members and manage team settings. If you are the one who created the team, you should have been automatically assigned as an admin. If this is not the case, it's likely a mistake that the W&B support team will need to correct.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/teams\n\nTeam Roles and Permissions\n\n\nSelect a team role when you invite colleagues to join a team. There are four team role options:\n\n\n* **Admin**: Team admins can add and remove other admins or team members. They have permissions to modify all projects and full deletion permissions. This includes, but is not limited to, deleting runs, projects, artifacts, and sweeps.\n* **Member**: A regular member of the team. A team member is invited by email by the team admin. A team member cannot invite other members. Team members can only delete runs and sweep runs created by that member. Suppose you have two members A and B. Member B moves a Run from team B\u2019s project to a different project owned by Member A. Member A can not delete the Run Member B moved to Member A\u2019s project. Only the member that creates the Run, or the team admin, can delete the run.\n* **Service (Enterprise-only feature)**: A service worker, an API key useful for using W&B with your run automation tools. If you use the API key from a service account for your team, make sure to set the environment variable **WANDB\\_USERNAME** to attribute runs to the correct user.\n* **View-Only (Enterprise-only feature)**: View-Only members can view assets within the team such as runs, reports, and workspaces. They can follow and comment on reports, but they can not create, edit, or delete project overview, reports, or runs. View-Only members do not have an API key.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/model_registry\/access_controls\n\nSet up access control\n\n\nThe following steps describe how to set up access controls for your team\u2019s model registry.\n\n\n1. Navigate to the W&B Model Registry app at <https:\/\/wandb.ai\/registry\/model>.\n2. Select the gear button on the top right of the page.\n3. Select the **Manage registry** admins button.\n4. Add members who should be allowed to add and remove protected aliases from model versions.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/teams\n\nSystem Settings (W&B Server Only)\n\n\nWith system permissions, you have the ability to manage members, create and modify teams, adjust system settings, and view user activity. These privileges enable you to effectively administer and maintain the W&B instance.\n\n\n\n\n| Permissions | View-Only | Team Member | Team Admin | System Admin |\n| --- | --- | --- | --- | --- |\n| Configure system settings |  |  |  | X |\n| Create\/delete teams |  |  |  | X |\n| View activity dashboard |  |  |  | X |\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/settings-page\/team-settings\n\nPrivacy\n\n\nNavigate to the **Privacy** section to change privacy settings. Only members with Administrative roles can modify privacy settings. Administrator roles can:\n\n\n* Force projects in the team to be private.\n* Enable code saving by default.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/teams\n\nTeam Settings\n\n\nTeam settings allow you to manage the settings for your team and its members. With these privileges, you can effectively oversee and organize your team within W&B.\n\n\n\n\n| Permissions | View-Only | Team Member | Team Admin |\n| --- | --- | --- | --- |\n| Add team members |  |  | X |\n| Remove team members |  |  | X |\n| Manage team settings |  |  | X |\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer provides appropriate troubleshooting steps and suggests that the user contact the support team which is the right way to resolve this issue and solve the user's problem here."}
{"question":"How can I add prediction-related data to a W&B run days after my training run (i.e. not in the same process\/moment that I did the training)? Is it possible to link prediction data to an external tool such as BigQuery and still get rich visualizations in W&B?","answer":"To add prediction-related data to a W&B run after your training run, you can create a new run for inference and associate it with the previous training run using W&B's API. Here's how you can do it:\n\n1. **Initialize a New Run for Inference**: Start a new W&B run for the inference phase. You can set the `group`, `job_type`, or `tags` to link it logically with your training run.\n\n```python\nimport wandb\n\n# Initialize a new W&B run for inference\ninference_run = wandb.init(\n    project=\"your_project_name\",\n    job_type=\"inference\",\n    tags=[\"inference\", \"post-training\"]\n)\n```\n\n2. **Log Predictions**: Use the W&B API to log predictions as artifacts or directly to a W&B Table.\n\n```python\n# Assuming you have predictions and ground truth data\npredictions = ...\nground_truth = ...\n\n# Log predictions to a W&B Table\ntable = wandb.Table(columns=[\"predictions\", \"ground_truth\"])\nfor pred, gt in zip(predictions, ground_truth):\n    table.add_data(pred, gt)\n\nwandb.log({\"predictions_table\": table})\n```\n\n3. **Associate with Training Run**: If you want to link this inference run with a specific training run, you can reference the training run's ID.\n\n```python\n# Optionally, you can use the training run's ID to link them\ntraining_run_id = \"your_training_run_id\"\nwandb.config.update({\"training_run_id\": training_run_id})\n```\n\n4. **Finish the Inference Run**: Once you've logged all the necessary data, you can finish the inference run.\n\n```python\n# Finish the W&B run\ninference_run.finish()\n```\n\nTo link prediction data to an external tool such as BigQuery and still get rich visualizations in W&B, you can:\n\n1. **Export Data to BigQuery**: Use BigQuery's API or tools to export your prediction data to a BigQuery table.\n\n2. **Query Data from BigQuery**: Write a script or use a tool to query the prediction data from BigQuery.\n\n3. **Log Data to W&B**: Use the retrieved data to create a W&B Table or other visualizations and log them to a W&B run as shown above.\n\nBy following these steps, you can enrich your W&B project with post-training inference data and maintain a clear lineage between training and inference runs. Additionally, by integrating with tools like BigQuery, you can leverage the power of big data analytics while still benefiting from the visualization and tracking capabilities of W&B.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/ultralytics\n\nVisualizing Prediction Results\n\n\nThis section demonstrates a typical workflow of using an Ultralytics model for inference and visualizing the results using W&B.\n\n\nYou can try out the code in Google Colab: Open in Colab.\n\n\nYou can also check out about the integration in this report: Supercharging Ultralytics with W&B\n\n\nIn order to use the W&B integration with Ultralytics, we need to import the `wandb.integration.ultralytics.add_wandb_callback` function.\n\n\n\n```\nimport wandb\nfrom wandb.integration.ultralytics import add_wandb_callback\n\nfrom ultralytics.engine.model import YOLO\n\n```\n\nNow, let us download a few images to test the integration on. You can use your own images, videos or camera sources. For more information on inference sources, you can check out the official docs.\n\n\n\n```\n!wget https:\/\/raw.githubusercontent.com\/wandb\/examples\/ultralytics\/colabs\/ultralytics\/assets\/img1.png\n!wget https:\/\/raw.githubusercontent.com\/wandb\/examples\/ultralytics\/colabs\/ultralytics\/assets\/img2.png\n!wget https:\/\/raw.githubusercontent.com\/wandb\/examples\/ultralytics\/colabs\/ultralytics\/assets\/img4.png\n!wget https:\/\/raw.githubusercontent.com\/wandb\/examples\/ultralytics\/colabs\/ultralytics\/assets\/img5.png\n\n```\n\nNext, we initialize a W&B run using `wandb.init`.\n\n\n\n```\n# Initialize W&B run\nwandb.init(project=\"ultralytics\", job_type=\"inference\")\n\n```\n\nNext, we initialize the `YOLO` model of our choice, and invoke the `add_wandb_callback` function on it before performing inference with the model. This would ensure that when we perform inference, it would automatically log the images overlaid with our interactive overlays for computer vision tasks along with additional insights in a `wandb.Table`.\n\n\n\n```\nmodel_name = 'yolov8n' #@param {type:\"string\"}\n\n# Initialize YOLO Model\nmodel = YOLO(f\"{model_name}.pt\")\n\n# Add W&B callback for Ultralytics\nadd_wandb_callback(model, enable_model_checkpointing=True)\n\n# Perform prediction which automatically logs to a W&B Table\n# with interactive overlays for bounding boxes, segmentation masks\nmodel([\".\/assets\/img1.jpeg\", \".\/assets\/img3.png\", \".\/assets\/img4.jpeg\", \".\/assets\/img5.jpeg\"])\n\n# Finish the W&B run\nwandb.finish()\n\n```\n\nNote: We do not need to explicitly initialize a run using `wandb.init()` in case of a training or fine-tuning workflow. However, tt is necessary to explicitly create a run, if the code only involves prediction.\n\n\nHere\u2019s how the interactive bbox overlay looks:\n\n\n\n> WandB Image Overlay\n\n\n\nYou can fine more information on the W&B image overlays here.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/reports\/\n\nCollaborative Reports\n\n\n\nUse W&B Reports to organize Runs, embed and automate visualizations, describe your findings, and share updates with collaborators. Easily export your report as a LaTeX zip file or convert the file to PDF.\n\n\n\nThe following image shows a section of a report created from metrics that were logged to W&B over the course of training. \n\n\n\nView the report where the above image was taken from here.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/openai-fine-tuning\n\nHow can I\u202forganize my runs?\n\n\nYour W&B runs are automatically organized and can be filtered\/sorted based on any configuration parameter such as job type, base model, learning rate, training filename and any other hyper-parameter.\n\n\nIn addition, you can rename your runs, add notes or create tags to group them.\n\n\nOnce you\u2019re satisfied, you can save your workspace and use it to create report, importing data from your runs and saved artifacts (training\/validation files).\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/keras\/Simple_Keras_Integration.ipynb\n\nTrain the model and log the predictions to a W&B Table\n\n\n\n```\n# Initialize a W&B run\nrun = wandb.init(\n    project = \"my-keras-project\",\n    config = configs\n)\n\nwandb_callbacks = [\n        WandbMetricsLogger(log_freq=10),\n        WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n        WandbClsEvalCallback(\n            validloader,\n            data_table_columns=[\"idx\", \"image\", \"ground_truth\"],\n            pred_table_columns=[\"epoch\", \"idx\", \"image\", \"ground_truth\", \"prediction\"]\n        ) \n    ]\n\n# Train your model\nmodel.fit(\n    trainloader,\n    epochs = configs[\"epochs\"],\n    validation_data = validloader,\n    callbacks = wandb_callbacks\n)\n\n# Close the W&B run\nrun.finish()\n\n```\n\nClick on the **W&B project page** link above to see your live results.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/\n\nArtifacts\n\n\n\nUse W&B Artifacts to track and version any serialized data as the inputs and outputs of your W&B Runs. For example, a model training run might take in a dataset as input and trained model as output. In addition to logging hyper-parameters and metadata to a run, you can use an artifact to log the dataset used to train the model as input and the resulting model checkpoints as outputs. You will always be able answer the question \u201cwhat version of my dataset was this model trained on\u201d.\n\n\nIn summary, with W&B Artifacts, you can:  \n\n\\* View where a model came from, including data it was trained on.  \n\n\\* Version every dataset change or model checkpoint.  \n\n\\* Easily reuse models and datasets across your team.\n\n\n\nThe diagram above demonstrates how you can use artifacts throughout your entire ML workflow; as inputs and outputs of runs.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"While this does not take into account updating values or Tables and artifacts the answer sufficient given the context.\n\nThe answer correctly outlines the steps for adding prediction-related data to a W&B run after training has been completed. It aligns with the documented functionality, suggesting to initialize a new run, log predictions, associate with the training run if needed, and finally finish the run. The answer also correctly addresses the possibility of integrating with BigQuery and logging the queried data back to a W&B run for visualization, thus staying true to the document's mention of W&B's ability to organize, track, and visualize data throughout an ML workflow.\n"}
{"question":"On my laptop, wandb is configured to log in to my company's wandb. However, I want to use my personal wandb account for my personal projects. Even after wandb login --relogin, it will still try to go to company.wandb.ai instead of api.wandb.ai. How can I solve this?\n\nEven better, is there a good way to handle two wandb accounts? I cannot give details about how I set my company wandb.","answer":"To switch between your company's W&B instance and the public W&B cloud, you need to specify the host you're logging into. This is because your company might be using a private instance of W&B, which has a different host address than the public W&B cloud.\n\nHere's how you can log in to your personal account on the public W&B cloud:\n\n1. Open a terminal on your laptop.\n2. Run the following command to log in to the public W&B cloud:\n\n```bash\nwandb login --relogin --host=https:\/\/api.wandb.ai\n```\n\nThis command forces a relogin (`--relogin`) and specifies the host of the public W&B cloud (`--host=https:\/\/api.wandb.ai`).\n\nTo handle two W&B accounts (your personal and your company's), you can use different API keys and specify the appropriate host when logging in. Here's a general approach:\n\n1. Log out of the current session:\n\n```bash\nwandb logout\n```\n\n2. Log in to the desired account by specifying the host and entering the corresponding API key when prompted:\n\nFor your personal account:\n\n```bash\nwandb login --relogin --host=https:\/\/api.wandb.ai\n```\n\nFor your company's account:\n\n```bash\nwandb login --relogin --host=https:\/\/company.wandb.ai\n```\n\nReplace `https:\/\/company.wandb.ai` with the actual host address of your company's W&B instance.\n\nTo make this process smoother, you can set environment variables to store your personal and company API keys. Then, you can create simple scripts or aliases in your shell configuration file (e.g., `.bashrc` or `.zshrc`) to switch between accounts.\n\nFor example, you could add the following to your `.bashrc` or `.zshrc`:\n\n```bash\nalias wandb_login_personal=\"export WANDB_API_KEY=your_personal_api_key && wandb login --relogin --host=https:\/\/api.wandb.ai\"\nalias wandb_login_company=\"export WANDB_API_KEY=your_company_api_key && wandb login --relogin --host=https:\/\/company.wandb.ai\"\n```\n\nReplace `your_personal_api_key` and `your_company_api_key` with your actual API keys. After adding these aliases, you can simply run `wandb_login_personal` or `wandb_login_company` in your terminal to switch between accounts.\n\nRemember to keep your API keys secure and not to share them publicly.","context":"Source:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/scikit\/w-b-k-means-clustering.ipynb\n\nProject Configuration using **`wandb.config`**\n\n\n\n```\nimport os\nimport wandb\n\n```\n\nlog to your weights and biases account\n\n\n\n```\nwandb.login()\n\n```\n\n\n```\n# Initialize the run\nrun = wandb.init(project='simple-scikit')\n\n# Feel free to change these and experiment !!\nconfig = wandb.config\nconfig.seed = 42\nconfig.n_clusters = 3\nconfig.dataset = 'iris'\nconfig.labels=['Setosa', 'Versicolour', 'Virginica']\n\n# Set random seed\nnp.random.seed(config.seed)\n\n# Update the config\nwandb.config.update(config)\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/datasets-predictions\/W&B_Dataset_Visualization.ipynb\n\nLogin to wandb\n\n\n\n```\n# default project name where results will be logged\nWANDB_PROJECT = \"dsviz-demo-colab\"\nNUM_EXAMPLES = 50\n\n```\n\n\n```\nwandb.login()\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/ref\/cli\/wandb-login\n\nwandb login\n\n\n**Usage**\n\n\n`wandb login [OPTIONS] [KEY]...`\n\n\n**Summary**\n\n\nLogin to Weights & Biases\n\n\n**Options**\n\n\n\n\n| **Option** | **Description** |\n| --- | --- |\n| \u2013cloud | Login to the cloud instead of local |\n| \u2013host | Login to a specific instance of W&B |\n| \u2013relogin | Force relogin if already logged in. |\n| \u2013anonymously | Log in anonymously |\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/quickstart\n\nCommon Questions\n\n\n**Where do I find my API key?**  \n\nOnce you\u2019ve signed in to www.wandb.ai, the API key will be on the Authorize page.\n\n\n**How do I use W&B in an automated environment?**  \n\nIf you are training models in an automated environment where it\u2019s inconvenient to run shell commands, such as Google\u2019s CloudML, you should look at our guide to configuration with Environment Variables.\n\n\n**Do you offer local, on-prem installs?**  \n\nYes, you can privately host W&B locally on your own machines or in a private cloud, try this quick tutorial notebook to see how. Note, to login to wandb local server you can set the host flag to the address of the local instance. \n\n\n**How do I turn off wandb logging temporarily?**  \n\nIf are testing code and want to disable wandb syncing, set the environment variable `WANDB_MODE=offline`.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/tutorials\/xgboost\n\nLog in to W&B\n\n\nLog in to Weights and Biases \n\n\n\n```\nimport wandb\n\nwandb.login()\n\nWANDB_PROJECT = \"vehicle_loan_default\"\n\n```\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The provided answer suggests editing the shell configuration files to handle multiple wandb accounts with aliases, which is not secure as hardcoding the API keys exposes them. The user specifically mentioned that they cannot give details about how they set their company's wandb, which implies they may not be able to share the wandb instance URL or other settings publicly. The correct approach would be to use environment variables to switch between the different wandb configurations without hardcoding sensitive data or to use the wandb login command with different API keys as needed."}
{"question":"I want to add a new columns to a wandb.Table with an operation involving other columns, what operations can I use and how to actually do this?","answer":"In `wandb.Table`, you can add new columns by performing operations on existing columns using standard Python operations and then appending the results as a new column. The operations you can perform on the columns depend on the data types of the columns you're working with. For numerical columns, you can perform arithmetic operations such as addition, subtraction, multiplication, division, and more complex mathematical functions provided by Python libraries like `math` or `numpy`.\n\nHere's a step-by-step guide on how to add a new column to a `wandb.Table` with an operation involving other columns:\n\n1. Initialize your W&B run using `wandb.init()`.\n2. Create a `wandb.Table` and log your initial data.\n3. Perform the desired operation on the existing columns to create a new list or array of values for the new column.\n4. Add the new column to the `wandb.Table` using the `add_column` method.\n5. Log the updated table to W&B.\n\nHere's an example code snippet that demonstrates these steps:\n\n```python\nimport wandb\n\n# Initialize a new run\nwandb.init(project=\"my_project\", name=\"add_column_example\")\n\n# Create initial tabular data\ntable = wandb.Table(columns=[\"Column A\", \"Column B\"])\ntable.add_data(1, 2)\ntable.add_data(3, 4)\ntable.add_data(5, 6)\n\n# Perform an operation on existing columns (e.g., sum of Column A and Column B)\nnew_column_data = [row[0] + row[1] for row in table.data]\n\n# Add the new column to the table\ntable.add_column(\"Sum of A and B\", new_column_data)\n\n# Log the updated table to W&B\nwandb.log({\"my_table\": table})\n\n# Finish the run\nwandb.finish()\n```\n\nIn this example, we added a new column called \"Sum of A and B\" to the `wandb.Table` by summing the values of \"Column A\" and \"Column B\". You can replace the operation with any other operation that is suitable for your data and use case.\n\nRemember that the operations you can perform will depend on the data types of the columns. If you're working with strings, you might concatenate them or perform other string operations. If you're working with timestamps, you might calculate time differences, and so on. Always ensure that the operation is valid for the data types of the columns you're working with.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/app\/pages\/project-page\n\nTable operations\n\n\nUse the W&B App to sort, filter, and group your W&B Tables. \n\n\n\n\nSort all rows in a Table by the value in a given column. \n\n\n1. Hover your mouse over the column title. A kebob menu will appear (three vertical docs).\n2. Select on the kebob menu (three vertical dots).\n3. Choose **Sort Asc** or **Sort Desc** to sort the rows in ascending or descending order, respectively.\n\n\n\nThe preceding image demonstrates how to view sorting options for a Table column called `val_acc`.\n\n\n\nFilter all rows by an expression with the **Filter** button on the top left of the dashboard. \n\n\n\nSelect **Add filter** to add one or more filters to your rows. Three dropdown menus will appear. From left to right the filter types are based on: Column name, Operator , and Values\n\n\n\n\n|  | Column name | Binary relation | Value |\n| --- | --- | --- | --- |\n| Accepted values | String | =, \u2260, \u2264, \u2265, IN, NOT IN, | Integer, float, string, timestamp, null |\n\n\nThe expression editor shows a list of options for each term using autocomplete on column names and logical predicate structure. You can connect multiple logical predicates into one expression using \u201cand\u201d or \u201cor\u201d (and sometimes parentheses).\n\n\n  \n\nThe preceding image shows a filter that is based on the `val_loss` column. The filter shows runs with a validation loss less than or equal to 1.\n\n\n\nGroup all rows by the value in a particular column with the **Group by** button in a column header. \n\n\n\nBy default, this turns other numeric columns into histograms showing the distribution of values for that column across the group. Grouping is helpful for understanding higher-level patterns in your data.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/prompts\/W&B_Prompts_with_Custom_Columns.ipynb\n\nLangChain Context Manager\n\n\nDepending on your use case, you might instead prefer to use a context manager to manage your logging to W&B.\n\n\n**\u2728 New: Custom columns** can be logged directly to W&B to display in the same Trace Table with this snippet:\n\n\n\n```\nimport wandb\nwandb.log(custom_metrics_dict, commit=False})\n\n```\n\nUse `commit=False` to make sure that metadata is logged to the same row of the Trace Table as the LangChain output.\n\n\n\n```\nfrom langchain.callbacks import wandb_tracing_enabled\nimport wandb # To enable custom column logging with wandb.run.log()\n\n# unset the environment variable and use a context manager instead\nif \"LANGCHAIN_WANDB_TRACING\" in os.environ:\n    del os.environ[\"LANGCHAIN_WANDB_TRACING\"]\n\n# enable tracing using a context manager\nwith wandb_tracing_enabled():\n  for i in range (10):\n    # Log any custom columns you'd like to add to the Trace Table\n    wandb.log({\"custom_column\": i}, commit=False)\n    try:\n      math_agent.run(f\"What is {i} raised to .123243 power?\")  # this should be traced\n    except:\n      pass\n\nmath_agent.run(\"What is 2 raised to .123243 power?\")  # this should not be traced\n\n```\n\n\n```\n'1.0891804557407723'\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/log-tables\n\nAdvanced: Artifact Tables\n\n\nUse `artifact.add()` to log tables to the Artifacts section of your run instead of the workspace. This could be useful if you have a dataset that you want to log once and then reference for future runs. \n\n\n\n```\nrun = wandb.init(project=\"my_project\")\n# create a wandb Artifact for each meaningful step\ntest_predictions = wandb.Artifact(\"mnist_test_preds\", type=\"predictions\")\n\n# [build up your predictions data as above]\ntest_table = wandb.Table(data=data, columns=columns)\ntest_predictions.add(test_table, \"my_test_key\")\nrun.log_artifact(test_predictions)\n\n```\n\nRefer to this Colab for a detailed example of artifact.add() with image data and this Report for an example of how to use Artifacts and Tables to version control and deduplicate tabular data.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/wandb-log\/Log_(Almost)_Anything_with_W&B_Media.ipynb\n\nLog tables\n\n\n\n```\n# Initialize a new run\nwandb.init(project=\"visualize-predictions\", name=\"tables\")\n\n# Create tabular data, method 1\ndata = [[\"I love my phone\", \"1\", \"1\"],[\"My phone sucks\", \"0\", \"-1\"]]\nwandb.log({\"a_table\": wandb.Table(data=data, columns=[\"Text\", \"Predicted Label\", \"True Label\"])})\n\n# Create tabular data, method 2\ntable = wandb.Table(columns=[\"Text\", \"Predicted Label\", \"True Label\"])\ntable.add_data(\"I love my phone\", \"1\", \"1\")\ntable.add_data(\"My phone sucks\", \"0\", \"-1\")\nwandb.log({\"another_table\": table})\n\nwandb.finish()\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/ref\/python\/artifact\n\nExamples:\n\n\nBasic usage:\n\n\n\n```\nartifact = wandb.Artifact(\"my_table\", type=\"dataset\")\ntable = wandb.Table(\n    columns=[\"a\", \"b\", \"c\"], data=[(i, i * 2, 2**i) for i in range(10)]\n)\nartifact.add(table, \"my_table\")\n\nwandb.log_artifact(artifact)\n\n```\n\nRetrieve an object:\n\n\n\n```\nartifact = wandb.use_artifact(\"my_table:latest\")\ntable = artifact.get(\"my_table\")\n\n```\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The provided answer accurately represents how to programmatically add a new column to a wandb.Table by performing operations with other columns using Python and then using the 'add_column' method to append the result. \nHowever, it's unclear whether the intention of the question was to do it via UI. It might be prudent to ask a follow-up question in such cases and clarify the intent of the user."}
{"question":"Hey there, I just recently starting using the `WANDB_WATCH=\"all\"` environment variable and I'm trying to find an intelligent way of viewing all these different values on the dashboard. Is there some type of high-level view? In particular I'd like to see how gradients\/activations vary across layers for the `llama` model. How do I see outliers across the model weights? Are there any references\/examples that you could point me to?","answer":"When you set `WANDB_WATCH=\"all\"`, Weights & Biases (W&B) logs histograms of both gradients and parameters for each layer of your model. This can generate a lot of data, especially for large models like `llama`. To view and analyze this data effectively on the W&B dashboard, you can use several features:\n\n1. **Custom Charts**: You can create custom charts on the W&B dashboard to visualize gradients and parameters. This allows you to focus on specific layers or aspects of the model that are of interest to you.\n\n2. **Grouping and Filtering**: You can group and filter your runs based on different criteria. This can help you compare different experiments or focus on a particular subset of your data.\n\n3. **Histograms**: W&B automatically generates histograms for logged gradients and parameters. You can view these histograms to understand the distribution of values across different layers.\n\n4. **Outliers**: To detect outliers in model weights or gradients, you can use the histograms and set thresholds to identify any unusual spikes or dips in the distributions.\n\n5. **Workspace**: W&B provides a workspace where you can organize your visualizations in a way that makes sense for your analysis. You can create sections for different types of visualizations, such as gradients, activations, and weights.\n\n6. **Reports**: You can create reports in W&B to document your findings and share insights with your team. Reports can include narrative, visualizations, and code, making them a comprehensive way to communicate your analysis.\n\nFor references and examples, you can check the following:\n\n- **W&B Gallery**: The W&B Gallery (https:\/\/wandb.ai\/gallery) contains a collection of projects and reports from the community. You can find examples of how other users visualize gradients and parameters.\n\n- **W&B Documentation**: The W&B documentation (https:\/\/docs.wandb.ai\/) provides guides and tutorials on how to use different features of the platform, including how to create custom visualizations.\n\n- **W&B Community Forum**: The W&B Community Forum (https:\/\/community.wandb.ai\/) is a place where you can ask questions and share knowledge with other W&B users. You can search for discussions related to visualizing gradients and parameters.\n\n- **W&B Examples**: The W&B GitHub repository (https:\/\/github.com\/wandb\/examples) contains example notebooks and scripts that demonstrate how to use W&B with various machine learning frameworks.\n\nRemember to use the `global_step` as the x-axis when creating charts to correctly superimpose metrics logged at different stages, as mentioned in the context information.\n\nIf you need further assistance, you can always reach out to W&B support at support@wandb.com for more personalized help.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/huggingface\n\nAdditional W&B settings\n\n\nFurther configuration of what is logged with `Trainer` is possible by setting environment variables. A full list of W&B environment variables can be found here.\n\n\n\n\n| Environment Variable | Usage |\n| --- | --- |\n| `WANDB_PROJECT` | Give your project a name (`huggingface` by default) |\n| `WANDB_LOG_MODEL` | Log the model as artifact at the end of training (`false` by default) |\n| `WANDB_WATCH` | Set whether you\u2019d like to log your models gradients, parameters or neither* `false` (default): No gradient or parameter logging\n* `gradients`: Log histograms of the gradients\n* `all`: Log histograms of gradients and parameters\n |\n| `WANDB_DISABLED` | Set to `true` to disable logging entirely (`false` by default) |\n| `WANDB_SILENT` | Set to `true` to silence the output printed by wandb (`false` by default) |\n\n\n\n\n```\nWANDB_WATCH=all\nWANDB_SILENT=true\n\n```\n\n\n\n```\n%env WANDB_WATCH=all\n%env WANDB_SILENT=true\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/paddlepaddle\/paddledetection\/PaddleDetection_and_W&B_Your_one_stop_for_everything_object_detection.ipynb\n\nQuestions about W&B\u2753\n\n\nIf you have any questions about using W&B to track your model performance and predictions, please contact support@wandb.com\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/pytorch-lightning\/Supercharge_your_Training_with_Pytorch_Lightning_and_Weights_and_Biases.ipynb\n\nViewing the results on wandb.ai\n\n\nAmong the outputs from W&B,  \n\nyou will have noticed a few URLs.  \n\nOne of these is the  \n\nrun page,  \n\nwhich has a dashboard with all of the information logged in this run, complete with smart default charts  \n\nand more.  \n\nThe run page is printed both at the start and end of training, and ends with `lit-wandb\/runs\/{run_id}`.\n\n\n\n> \n> *Note*: When visiting your run page, it is recommended to use `global_step` as x-axis to correctly superimpose metrics logged in different stages.\n> \n> \n>\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/paddledetection\n\nUsing a config.yml file\n\n\nYou can also activate wandb via the config file. Add the wandb arguments to the config.yml file under the wandb header like so:\n\n\n\n```\nwandb:\n  project: MyProject\n  entity: MyTeam\n  save_dir: .\/logs\n\n```\n\nOnce you run your `train.py` file with Weights & Biases turned on, a link will be generated to bring you to your W&B dashboard:\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/lightning\n\nLog gradients, parameter histogram and model topology\n\n\nYou can pass your model object to `wandblogger.watch()` to monitor your models\u2019s gradients and parameters as you train. See the PyTorch Lightning **`WandbLogger` documentation** for a full description\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer is consistent with the provided document, highlighting using the 'WANDB_WATCH' environment variable and offering a detailed explanation of utilizing the Weights & Biases dashboard to view gradients, parameters, and potential outliers in model weights."}
{"question":"Wandb ignores --configs flag when running training script. Running a script I want to specify what config YAML to use. The documentation states you can specify what configuration file to load using the `--configs` flag, however running\n\n`python myscript.py --configs new-configs.yaml`\n\non the following script\n\n`import wandb\nwandb.init()\nprint(wandb.config)`\n\nI can see that the configuration contained in `config-defaults.yaml` is being loaded instead. Why is this happening?","answer":"The `--configs` flag is not a built-in feature of the `wandb` library or the `wandb.init()` function. The `wandb` library automatically loads configurations from a file named `config-defaults.yaml` if it exists, as mentioned in the documentation. However, it does not natively recognize the `--configs` flag to specify a different configuration file.\n\nIf you want to load a different configuration file, you have a couple of options:\n\n1. **Manually specify the configuration in your script**: You can manually read the desired YAML file and pass the configurations to `wandb.init()` using the `config` argument.\n\nHere's an example of how you can modify your script to read a specific YAML file:\n\n```python\nimport wandb\nimport yaml\nimport sys\n\n# Function to parse command line arguments for a custom config file\ndef parse_args():\n    config_path = 'config-defaults.yaml'  # default config path\n    for i, arg in enumerate(sys.argv):\n        if arg == '--configs' and i + 1 < len(sys.argv):\n            config_path = sys.argv[i + 1]\n    return config_path\n\n# Load the configuration from the specified YAML file\ndef load_config(config_path):\n    with open(config_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n\n# Parse command line arguments\nconfig_path = parse_args()\n\n# Load the configuration\nconfig = load_config(config_path)\n\n# Initialize wandb with the loaded configuration\nwandb.init(config=config)\n\n# Print the configuration to verify\nprint(wandb.config)\n```\n\nTo run this script with a custom configuration file, you would use the following command:\n\n```bash\npython myscript.py --configs new-configs.yaml\n```\n\n2. **Use a custom command-line argument parser**: If you want to make the `--configs` flag work as you described, you would need to implement argument parsing in your script using libraries like `argparse` or manually parsing `sys.argv`. The above example shows a simple way to manually parse the `--configs` argument.\n\nRemember to remove or rename the `config-defaults.yaml` file if you do not want it to be automatically loaded by `wandb`. If `wandb` finds a `config-defaults.yaml` file, it will load it unless you explicitly pass a different configuration to `wandb.init()`.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/technical-faq\/general\n\nWhat does `wandb.init` do to my training process?\n\n\nWhen `wandb.init()` is called from your training script an API call is made to create a run object on our servers. A new process is started to stream and collect metrics, thereby keeping all threads and logic out of your primary process. Your script runs normally and writes to local files, while the separate process streams them to our servers along with system metrics. You can always turn off streaming by running `wandb off` from your training directory, or setting the `WANDB_MODE` environment variable to `offline`.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/yolox\/Train_and_Debug_YOLOX_Models_with_Weights_&_Biases.ipynb\n\nTraining \ud83c\udfcb\ufe0f\n\n\nUsing wandb just requires configuring the command line argument `--logger wandb`. This automatically turns on the wandb logger for your experiment and further arguments can be added -\n\n\n1. `wandb-project`: To specify the project in which experiment is being run.\n2. `wandb-run`: The name of the wandb run\n3. `wandb-entity`: Entity which is starting the run\n4. `wandb-log_checkpoints`: True\/False to log model checkpoints to the wandb dashboard\n5. `wandb-num_eval_images`: Number of images from the validation set to be logged to wandb. Predictions corresponding to these can be visualized on the dashboard. No images are logged if the value is 0 and all are logged if the value is -1.\n\n\nand more!\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/launch\/launch-quickstart\/train.py\n\nwith wandb.init(config=config) as run:\n  \n  # Training setup\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/add-w-and-b-to-your-code\n\nTraining script with W&B Python SDK\n\nThe following code examples demonstrate how to add the W&B Python SDK into your code. If you start W&B Sweep jobs in the CLI, you will want to explore the CLI tab. If you start W&B Sweep jobs within a Jupyter notebook or Python script, explore the Python SDK tab.\n\n\n\n<Tabs\n  defaultValue=\"script\"\n  values={[\n    {label: 'Python script or Jupyter Notebook', value: 'script'},\n    {label: 'CLI', value: 'cli'},\n  ]}>\n  <TabItem value=\"script\">\n  To create a W&B Sweep, we added the following to the code example:\n\n1. Line 1: Import the Weights & Biases Python SDK.\n2. Line 6: Create a dictionary object where the key-value pairs define the sweep configuration. In the proceeding example, the batch size (`batch_size`), epochs (`epochs`), and the learning rate (`lr`) hyperparameters are varied during each sweep. For more information on how to create a sweep configuration, see Define sweep configuration.\n3. Line 19: Pass the sweep configuration dictionary to `wandb.sweep`. This initializes the sweep. This returns a sweep ID (`sweep_id`). For more information on how to initialize sweeps, see Initialize sweeps.\n4. Line 33: Use the `wandb.init()` API to generate a background process to sync and log data as a W&B Run.\n5. Line 37-39: (Optional) define values from `wandb.config` instead of defining hard coded values.\n6. Line 45: Log the metric we want to optimize with `wandb.log`. You must log the metric defined in your configuration. Within the configuration dictionary (`sweep_configuration` in this example) we defined the sweep to maximize the `val_acc` value).\n7. Line 54: Start the sweep with the `wandb.agent` API call. Provide the sweep ID (line 19), the name of the function the sweep will execute (`function=main`), and set the maximum number of runs to try to four (`count=4`). For more information on how to start W&B Sweep, see Start sweep agents.\n\n\n```python showLineNumbers\nimport wandb\nimport numpy as np\nimport random\n\n# Define sweep config\nsweep_configuration = {\n    \"method\": \"random\",\n    \"name\": \"sweep\",\n    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_acc\"},\n    \"parameters\": {\n        \"batch_size\": {\"values\": [16, 32, 64]},\n        \"epochs\": {\"values\": [5, 10, 15]},\n        \"lr\": {\"max\": 0.1, \"min\": 0.0001},\n    },\n}\n\n# Initialize sweep by passing in config.\n# (Optional) Provide a name of the project.\nsweep_id = wandb.sweep(sweep=sweep_configuration, project=\"my-first-sweep\")\n\n\n# Define training function that takes in hyperparameter\n# values from `wandb.config` and uses them to train a\n# model and return metric\ndef train_one_epoch(epoch, lr, bs):\n    acc = 0.25 + ((epoch \/ 30) + (random.random() \/ 10))\n    loss = 0.2 + (1 - ((epoch - 1) \/ 10 + random.random() \/ 5))\n    return acc, loss\n\n\ndef evaluate_one_epoch(epoch):\n    acc = 0.1 + ((epoch \/ 20) + (random.random() \/ 10))\n    loss = 0.25 + (1 - ((epoch - 1) \/ 10 + random.random() \/ 6))\n    return acc, loss\n\n\ndef main():\n    run = wandb.init()\n\n    # note that we define values from `wandb.config`\n    # instead of defining hard values\n    lr = wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n        train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n        val_acc, val_loss = evaluate_one_epoch(epoch)\n\n        wandb.log(\n            {\n                \"epoch\": epoch,\n                \"train_acc\": train_acc,\n                \"train_loss\": train_loss,\n                \"val_acc\": val_acc,\n                \"val_loss\": val_loss,\n            }\n        )\n\n\n# Start sweep job.\nwandb.agent(sweep_id, function=main, count=4)\n\n```\n\n\nTo create a W&B Sweep, we first create a YAML configuration file. The configuration file contains he hyperparameters we want the sweep to explore. In the proceeding example, the batch size (`batch_size`), epochs (`epochs`), and the learning rate (`lr`) hyperparameters are varied during each sweep.\n\n\n\n```\n# config.yaml\nprogram: train.py\nmethod: random\nname: sweep\nmetric:\n  goal: maximize\n  name: val_acc\nparameters:\n  batch_size: \n    values: [16,32,64]\n  lr:\n    min: 0.0001\n    max: 0.1\n  epochs:\n    values: [5, 10, 15]\n\n```\n\nFor more information on how to create a W&B Sweep configuration, see Define sweep configuration.\n\n\nNote that you must provide the name of your Python script for the `program` key in your YAML file.\n\n\nNext, we add the following to the code example:\n\n\n1. Line 1-2: Import the Wieghts & Biases Python SDK (`wandb`) and PyYAML (`yaml`). PyYAML is used to read in our YAML configuration file.\n2. Line 18: Read in the configuration file.\n3. Line 21: Use the `wandb.init()` API to generate a background process to sync and log data as a W&B Run. We pass the config object to the config parameter.\n4. Line 25 - 27: Define hyperparameter values from `wandb.config` instead of using hard coded values.\n5. Line 33-39: Log the metric we want to optimize with `wandb.log`. You must log the metric defined in your configuration. Within the configuration dictionary (`sweep_configuration` in this example) we defined the sweep to maximize the `val_acc` value.\n\n\n```python showLineNumbers  \n\nimport wandb  \n\nimport yaml  \n\nimport random  \n\nimport numpy as np\n\n\ndef train*one*epoch(epoch, lr, bs):  \n\n acc = 0.25 + ((epoch \/ 30) + (random.random() \/ 10))  \n\n loss = 0.2 + (1 - ((epoch - 1) \/ 10 + random.random() \/ 5))  \n\n return acc, loss\n\n\ndef evaluate*one*epoch(epoch):  \n\n acc = 0.1 + ((epoch \/ 20) + (random.random() \/ 10))  \n\n loss = 0.25 + (1 - ((epoch - 1) \/ 10 + random.random() \/ 6))  \n\n return acc, loss\n\n\ndef main():  \n\n # Set up your default hyperparameters  \n\n with open(\u201c.\/config.yaml\u201d) as file:  \n\n config = yaml.load(file, Loader=yaml.FullLoader)\n\n\n\n```\nrun = wandb.init(config=config)\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/config\n\nFile-Based Configs\n\n\nKey-value pairs are automatically passed to `wandb.config` if you create a file called `config-defaults.yaml`.\n\n\nThe proceeding code snippet demonstrates a sample `config-defaults.yaml` YAML file:\n\n\n\n```\n# config-defaults.yaml\n# sample config defaults file\nepochs:\n  desc: Number of epochs to train over\n  value: 100\nbatch_size:\n  desc: Size of each mini-batch\n  value: 32\n\n```\n\nYou can overwrite automatically passed by a `config-defaults.yaml`. To do so , pass values to the `config` argument of `wandb.init`.\n\n\nYou can also load different config files with the command line argument `--configs`.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer correctly identifies that the `--configs` flag is not a built-in feature of the wandb library and provides alternative methods to load a different YAML configuration file into `wandb.init`. The user-provided code snippet demonstrates how to manually read a YAML file and pass the configurations to wandb, which aligns with the official documentation that suggests using a `config-defaults.yaml` or passing values directly to the `wandb.init` config argument."}
{"question":"I want to submit a wandb Report to a conference, can I somehow anonymize the datA?","answer":"As of the last update I have, W&B (Weights & Biases) does not provide a direct feature specifically for anonymizing data in reports for the purpose of conference submissions. However, there are some steps you can take to anonymize your data before submitting it to a conference:\n\n1. **Anonymous Mode**: If you're sharing code and want others to run it and see the results without creating a W&B account, you can use Anonymous Mode by setting `wandb.init(anonymous=\"allow\")`. This allows for the creation of temporary anonymous accounts. However, this does not anonymize the data itself, just the user account.\n\n2. **Manual Anonymization**: Before creating your report, manually remove or obfuscate any sensitive information from your data. This could include names, usernames, identifiable metadata, or any other information that could be used to trace back to the original source.\n\n3. **Custom Privacy Settings**: You can adjust the privacy settings of your project and runs in W&B to control who can see them. For a conference submission, you might want to set the project to \"Public\" but ensure that no sensitive or identifiable information is included.\n\n4. **Contact W&B Support**: If you need to anonymize data for a specific use case, such as a conference submission, and you're not sure how to proceed, you can reach out to W&B support at support@wandb.com for assistance. They may be able to provide guidance or solutions tailored to your situation.\n\nRemember that when sharing results publicly, you should be careful not to share sensitive links or data. If you're trying to share results while hiding the author's identity, it's best to reach out to W&B support for advice on how to do this securely and effectively.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/anon\n\nAnonymous Mode\n\n\nAre you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first.\n\n\nAllow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)`\n\n\n:::info  \n\n**Publishing a paper?** Please cite W&B, and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com.  \n\n:::\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/anon\n\nExample usage\n\n\nTry the example notebook to see how anonymous mode works.\n\n\n\n```\nimport wandb\n\n# Start a run allowing anonymous accounts\nwandb.init(anonymous=\"allow\")\n\n# Log results from your training loop\nwandb.log({\"acc\": 0.91})\n\n# Mark the run as finished\nwandb.finish()\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/anon\n\nHow does someone without an account see results?\n\n\nIf someone runs your script and you have to set `anonymous=\"allow\"`:\n\n\n1. **Auto-create temporary account:** W&B checks for an account that\u2019s already signed in. If there\u2019s no account, we automatically create a new anonymous account and save that API key for the session.\n2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days.\n3. **Claim data when it\u2019s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\u2019t claim a run, it will be deleted after 7 days.\n\n\n:::caution  \n\n**Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\u2019re trying to share results publicly, but hide the author\u2019s identity, please contact us at support@wandb.com to share more about your use case.  \n\n:::\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/intro\/Report_API_Quickstart.ipynb\n\n\ud83d\ude80 Quickstart!\n\n\n\n```\nimport wandb.apis.reports as wr\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/technical-faq\/metrics-and-performance\n\nWill wandb slow down my training?\n\n\nW&B should have a negligible effect on your training performance if you use it normally. Normal use of wandb means logging less than once a second and logging less than a few megabytes of data at each step. W&B runs in a separate process and the function calls don\u2019t block, so if the network goes down briefly or there are intermittent read write issues on disk it should not affect your performance. It is possible to log a huge amount of data quickly, and if you do that you might create disk I\/O issues. If you have any questions, please don\u2019t hesitate to contact us.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer provides accurate advice on anonymizing data for conference submission by utilizing Anonymous Mode and other steps while highlighting that direct anonymization features for reports are not specified."}
{"question":"i am trying to figure out the best way to organize\/name\/group my wandb runs when doing hyperparam tuning with cross-validation.\n\nideally, i'd like to be able to see the average score across all the runs that are part of a CV set. and then see the best CV score across the whole hyperparam sweep. (i am using optuna, not wandb, to manage the hyperparam sweep)\n\nis there a recommended approach for this?","answer":"When using W&B (Weights & Biases) in conjunction with Optuna for hyperparameter tuning with cross-validation, you can organize your runs effectively by leveraging W&B's features such as projects, groups, tags, and notes. Here's a recommended approach to achieve your goals:\n\n1. **Projects**: Use a dedicated project for your hyperparameter sweep. This will allow you to compare all runs within the same context.\n   ```python\n   wandb.init(project=\"your_hyperparam_sweep_project\")\n   ```\n\n2. **Groups**: Group your runs by the hyperparameter configuration. Each group can represent one complete set of cross-validation folds for a particular hyperparameter set. This way, you can easily compare different configurations.\n   ```python\n   wandb.init(group=f\"config-{config_id}\")\n   ```\n   Here, `config_id` could be a unique identifier for each hyperparameter configuration generated by Optuna.\n\n3. **Tags**: Use tags to mark runs with specific characteristics, such as the fold number or whether it's part of the best-performing CV set.\n   ```python\n   wandb.init(tags=[f\"fold-{fold_number}\", \"best_cv_score\"])\n   ```\n\n4. **Notes**: Add notes to each run to describe any particularities or observations that might be relevant when reviewing the results.\n\n5. **Reports**: Create reports to summarize the findings of your hyperparameter sweep, including tables or graphs that show the average score across all CV sets and the best CV score.\n\n6. **Custom Metrics**: Since you want to see the average score across all runs in a CV set and the best CV score across the whole sweep, you can calculate these metrics manually and log them as custom metrics in W&B.\n   ```python\n   wandb.log({\"average_cv_score\": avg_score, \"best_cv_score\": best_score})\n   ```\n\n7. **Result Aggregation**: To see the average score across all runs that are part of a CV set, you can use W&B's grouping feature in the UI to aggregate results. For example, you can group by the hyperparameter configuration and then average the performance metric across the folds.\n\n8. **Analysis**: To find the best CV score across the entire hyperparameter sweep, you can use W&B's query and filtering capabilities in the UI to sort the runs by performance and select the top-performing configurations.\n\nBy following these steps, you can effectively organize and analyze your runs in W&B while using Optuna for the hyperparameter sweep. This setup will help you track and compare the performance of different hyperparameter sets and cross-validation folds, leading to more informed decisions about model selection.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/openai-fine-tuning\n\nHow can I\u202forganize my runs?\n\n\nYour W&B runs are automatically organized and can be filtered\/sorted based on any configuration parameter such as job type, base model, learning rate, training filename and any other hyper-parameter.\n\n\nIn addition, you can rename your runs, add notes or create tags to group them.\n\n\nOnce you\u2019re satisfied, you can save your workspace and use it to create report, importing data from your runs and saved artifacts (training\/validation files).\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/technical-faq\/metrics-and-performance\n\nBest practices to organize hyperparameter searches\n\n\nIf 10k runs per project (approx.) is a reasonable limit then our recommendation would be to set tags in `wandb.init()` and have a unique tag for each search. This means that you\u2019ll easily be able to filter the project down to a given search by clicking that tag in the Project Page in the Runs Table. For example `wandb.init(tags='your_tag')` docs for this can be found here.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/intro\/3_in_1_Intro_to_Weights_&_Biases_CV,_NLP_and_RL.ipynb\n\n\u270d\ufe0f W&B Best Practices\n\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/panels\/line-plot\/reference\n\nGrouping\n\n\nYou can aggregate all of the runs by turning on grouping, or group over an individual variable. You can also turn on grouping by grouping inside the table and the groups will automatically populate into the graph.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/tensorflow\/Simple_TensorFlow_Integration.ipynb\n\n\ud83d\udccf Best Practices\n\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group='experiment-1')`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer provides a strategy for organizing and naming runs in Weights & Biases (W&B) when doing hyperparameter tuning with cross-validation and using Optuna for hyperparameter sweeps. It accurately utilizes information from the provided document by suggesting the use of projects, groups, tags, and notes, which are W&B features described in the document. It also provides correct code snippets to illustrate the usage of these features in Python. There is no contradiction with the document, and the answer covers the key concepts related to organizing and analyzing runs for the scenario described in the question.\n"}
{"question":"can you tell why this error occurred while uploading the training results to the server:\nwandb: ERROR Error while calling W&B API: run lee086824\/Graduate_Project\/4iolqdaz was previously created and deleted; try a new run name (Response [409]>)","answer":"The error message you're encountering indicates that the run with the ID `4iolqdaz` under the project `Graduate_Project` in the workspace `lee086824` was previously created and then deleted. When a run is deleted from Weights & Biases (W&B), its ID is marked as used and cannot be reused for a new run. This is to maintain the integrity of the data and ensure that run IDs are unique within a project.\n\nThe HTTP response code `409` is a conflict error, which in this context means that there's a conflict with the run ID you're trying to use.\n\nTo resolve this issue, you should not specify the run ID manually when initializing a new run if the ID has been used before. Instead, allow W&B to automatically generate a new, unique run ID for you. You can do this by simply calling `wandb.init()` without the `id` parameter, or if you need to specify a custom name for the run, you can use the `name` parameter as shown below:\n\n```python\nwandb.init(name=\"a helpful readable run name\")\n```\n\nIf you are using W&B Sweeps or any other automated process that initializes runs, make sure that you are not setting the run ID manually or reusing IDs from deleted runs. This will prevent the conflict error and allow your training results to be uploaded successfully.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/distributed-training\n\nUse W&B Service to avoid common distributed training issues.\n\n\nThere are two common issues you might encounter when using W&B and distributed training:\n\n\n1. **Hanging at the beginning of training** - A `wandb` process can hang if the `wandb` multiprocessing interferes with the multiprocessing from distributed training.\n2. **Hanging at the end of training** - A training job might hang if the `wandb` process does not know when it needs to exit. Call the `wandb.finish()` API at the end of your Python script to tell W&B that the Run finished. The wandb.finish() API will finish uploading data and will cause W&B to exit.\n\n\nWe recommend using the `wandb service` to improve the reliability of your distributed jobs. Both of the preceding training issues are commonly found in versions of the W&B SDK where wandb service is unavailable.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/troubleshoot-sweeps\n\n`CommError, Run does not exist` and `ERROR Error uploading`\n\n\nYour W&B Run ID might be defined if these two error messages are both returned. As an example, you might have a similar code snippet defined somewhere in your Jupyter Notebooks or Python script:\n\n\n\n```\nwandb.init(id=\"some-string\")\n\n```\n\nYou can not set a Run ID for W&B Sweeps because W&B automatically generates random, unique IDs for Runs created by W&B Sweeps.\n\n\nW&B Run IDs need to be unique within a project.\n\n\nWe recommend you pass a name to the name parameter when you initialized W&B, if you want to set a custom name that will appear on tables and graphs. For example:\n\n\n\n```\nwandb.init(name=\"a helpful readable run name\")\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/technical-faq\/troubleshooting\n\nWhy is a run marked crashed in W&B when it\u2019s training fine locally?\n\n\nThis is likely a connection problem \u2014 if your server loses internet access and data stops syncing to W&B, we mark the run as crashed after a short period of retrying.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/tracking-faq\n\nMy run\u2019s state is \u201ccrashed\u201d on the UI but is still running on my machine. What do I do to get my data back?\n\n\nYou most likely lost connection to your machine while training. You can recover your data by running [`wandb sync [PATH_TO_RUN]`](..\/..\/ref\/cli\/wandb-sync.md). The path to your run will be a folder in your `wandb` directory corresponding to the Run ID of the run in progress.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/pytorch-lightning\/Supercharge_your_Training_with_Pytorch_Lightning_and_Weights_and_Biases.ipynb\n\nViewing the results on wandb.ai\n\n\nAmong the outputs from W&B,  \n\nyou will have noticed a few URLs.  \n\nOne of these is the  \n\nrun page,  \n\nwhich has a dashboard with all of the information logged in this run, complete with smart default charts  \n\nand more.  \n\nThe run page is printed both at the start and end of training, and ends with `lit-wandb\/runs\/{run_id}`.\n\n\n\n> \n> *Note*: When visiting your run page, it is recommended to use `global_step` as x-axis to correctly superimpose metrics logged in different stages.\n> \n> \n>\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The explanation correctly describes the error message in question. It provides steps to resolve the issue by not specifying a run ID manually, which aligns with the guidance given in the documentation. It provides the correct context and solution for the 409 error encountered when trying to reuse a run ID that was previously created and deleted.\n\n"}
{"question":"why am I getting a table instead of confusion matrix chart using     wandb.log({\"confusion_matrix_normalized\": wandb.plot.confusion_matrix(\n        y_true=y_test,\n        probs=y_probas,\n        class_names=text_labels\n    )})","answer":"When using `wandb.log` to log a confusion matrix, you should typically see a confusion matrix chart in your Weights & Biases dashboard. If you're getting a table instead of a chart, there could be a few reasons for this:\n\n1. **Data Format Issues**: Ensure that `y_test` and `y_probas` are in the correct format. `y_test` should be a list of true class indices, and `y_probas` should be a 2D list or array of probabilities for each class.\n\n2. **Class Names Mismatch**: The `class_names` parameter should match the number of classes represented in `y_probas`. If there's a mismatch, the confusion matrix might not be generated correctly.\n\n3. **API Changes**: If there have been recent changes to the Weights & Biases API, the way you log a confusion matrix might have changed. Always check the latest documentation for any updates or changes in the API.\n\n4. **Visualization Limitations**: There might be limitations or conditions under which Weights & Biases defaults to showing a table instead of a chart. This could be due to the size of the matrix or other visualization constraints.\n\n5. **Incorrect Usage**: Double-check that you're using the `wandb.plot.confusion_matrix` function correctly. The parameters should be passed as named arguments, and the function should return a plot object that can be logged.\n\n6. **W&B Version**: Make sure you are using the latest version of the Weights & Biases library. Sometimes, bugs or issues in older versions can cause unexpected behavior.\n\n7. **Browser or Interface Issues**: Sometimes, the issue might be with the browser or the Weights & Biases interface not rendering the visualization correctly. Try refreshing the page or clearing the browser cache.\n\nHere's a checklist to troubleshoot the issue:\n\n- Verify that `y_test` and `y_probas` are in the correct format and contain valid data.\n- Ensure that `class_names` is a list of strings that correspond to the class labels and matches the dimensions of `y_probas`.\n- Check the Weights & Biases documentation for any updates to the `wandb.plot.confusion_matrix` function.\n- Update Weights & Biases to the latest version using `pip install --upgrade wandb`.\n- Try logging a small dummy confusion matrix to see if the issue persists.\n- Look for any error messages or warnings in your console or Weights & Biases interface that might provide more information.\n\nIf you've gone through these steps and still face issues, consider reaching out to Weights & Biases support or checking community forums for help. There might be specific edge cases or bugs that are affecting the visualization.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/technical-faq\/general\n\nWhat is the difference between `.log()` and `.summary`?\n\n\nThe summary is the value that shows in the table while the log will save all the values for plotting later.\n\n\nFor example, you might want to call `wandb.log` every time the accuracy changes. Usually, you can just use .log. `wandb.log()` will also update the summary value by default unless you have set the summary manually for that metric\n\n\nThe scatterplot and parallel coordinate plots will also use the summary value while the line plot plots all of the values set by .log\n\n\nThe reason we have both is that some people like to set the summary manually because they want the summary to reflect for example the optimal accuracy instead of the last accuracy logged.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/pyg\/pointnet-classification\/00_eda.ipynb\n\nLog Data to `wandb.Table`\n\n\nWe now log the dataset using a Weights & Biases Table, which includes visualizing the individual point clouds as W&B\u2019s interactive 3D visualization format `wandb.object3D`. We also log the frequency distribution of the classes in the dataset using `wandb.plot`.\n\n\n\n```\ntable = wandb.Table(columns=[\"Model\", \"Class\", \"Split\"])\ncategory_dict = {key: 0 for key in config.categories}\nfor idx in tqdm(range(len(train_dataset[:20]))):\n    point_cloud = wandb.Object3D(train_dataset[idx].pos.numpy())\n    category = config.categories[int(train_dataset[idx].y.item())]\n    category_dict[category] += 1\n    table.add_data(\n        point_cloud,\n        category,\n        \"Train\"\n    )\n\ndata = [[key, category_dict[key]] for key in config.categories]\nwandb.log({\n    f\"{config.modelnet_dataset_alias} Class-Frequency Distribution\" : wandb.plot.bar(\n        wandb.Table(data=data, columns = [\"Class\", \"Frequency\"]),\n        \"Class\", \"Frequency\",\n        title=f\"{config.modelnet_dataset_alias} Class-Frequency Distribution\"\n    )\n})\n\n```\n\n\n```\ntable = wandb.Table(columns=[\"Model\", \"Class\", \"Split\"])\ncategory_dict = {key: 0 for key in config.categories}\nfor idx in tqdm(range(len(val_dataset[:100]))):\n    point_cloud = wandb.Object3D(val_dataset[idx].pos.numpy())\n    category = config.categories[int(val_dataset[idx].y.item())]\n    category_dict[category] += 1\n    table.add_data(\n        point_cloud,\n        category,\n        \"Test\"\n    )\nwandb.log({config.modelnet_dataset_alias: table})\n\ndata = [[key, category_dict[key]] for key in config.categories]\nwandb.log({\n    f\"{config.modelnet_dataset_alias} Class-Frequency Distribution\" : wandb.plot.bar(\n        wandb.Table(data=data, columns = [\"Class\", \"Frequency\"]),\n        \"Class\", \"Frequency\",\n        title=f\"{config.modelnet_dataset_alias} Class-Frequency Distribution\"\n    )\n})\n\n```\n\n\n```\nwandb.finish()\n\n```\n\nNext, you can check out the following notebook to learn how to compare different sampling strategies in PyTorch Geometric using Weights & Biases\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/logging-faqs\n\nWhy is nothing showing up in my graphs?\n\n\nIf you\u2019re seeing \u201cNo visualization data logged yet\u201d that means that we haven\u2019t gotten the first `wandb.log` call from your script yet. This could be because your run takes a long time to finish a step. If you\u2019re logging at the end of each epoch, you could log a few times per epoch to see data stream in more quickly.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/keras\/Legacy_Keras_pipeline_with_Weights_and_Biases.ipynb\n\n\ud83c\udf33 Explore the Dataset using W&B Tables\n\n\nAs TensorFlow\/Keras users you might be familar with the `show_batch` function. Or you might have written some `matplotlib` based code to visualize few batches of dataset. This is good for quick inspection of the dataset but for most real life scenario it\u2019s not enough. \n\n\nHere we will use W&B Tables (`wandb.Table`) to log the training data and visualize and query iteractively with W&B. As the name suggests it is a table of data specified by you. Check out more on Tables here.\n\n\nYou can log data to W&B Tables row wise or column wise. In the section below, **we have created the table column wise**. Use `add_column` to define the name of the column and provide array of data associated with that column. Simply adding array of images will not render in the W&B Tables UI. You will have to wrap each image array with `wandb.Image`. To do so, `add_computed_columns` is used. You can learn about these methods here.\n\n\nFinally, note that W&B Tables is built on top of **W&B Artifacts**, which can be viewed as a file (usually for dataset and models) storage system in W&B. In this section, we have explicitly initialized an artifact using `wandb.Artifact` and have added both the `train_table` and `validation_table` to the artifact. Alternatively, we could have prepared the table and logged it using `wandb.log`. Here\u2019s a quick example if you are interested. \n\n\nIf you want to log the entire dataset place a tick in the `log_full` checkbox. Note that we are logging the entire validation data.\n\n\n\n```\n# For demonstration purposes\nlog_full = False #@param {type:\"boolean\"}\n\nif log_full:\n    log_train_samples = len(train_images)\nelse:\n    log_train_samples = 1000 \n\nprint(f'Number of train images : {log_train_samples} to be logged')\n\n```\n\n\n```\n%%time\n\n# Initialize a new W&B run\nrun = wandb.init(project='medmnist-bloodmnist', group='viz_data')\n\n# Intialize a W&B Artifacts\nds = wandb.Artifact(\"medmnist_bloodmnist_dataset\", \"dataset\")\n\n# Initialize an empty table\ntrain_table = wandb.Table(columns=[], data=[])\n# Add training data\ntrain_table.add_column('image', train_images[:log_train_samples])\n# Add training label_id\ntrain_table.add_column('label_id', train_labels[:log_train_samples])\n# Add training class names\ntrain_table.add_computed_columns(lambda ndx, row:{\n    \"images\": wandb.Image(row[\"image\"]),\n    \"class_names\": configs['class_names'][str(row[\"label_id\"])]\n    })\n\n# Add the table to the Artifact\nds['train_data'] = train_table\n\n# Let's do the same for the validation data\nvalid_table = wandb.Table(columns=[], data=[])\nvalid_table.add_column('image', valid_images)\nvalid_table.add_column('label_id', valid_labels)\nvalid_table.add_computed_columns(lambda ndx, row:{\n    \"images\": wandb.Image(row[\"image\"]),\n    \"class_name\": configs['class_names'][str(row[\"label_id\"])]\n    })\nds['valid_data'] = valid_table\n\n# Save the dataset as an Artifact\nds.save()\n\n# Finish the run\nwandb.finish()\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/keras\/Simple_Keras_Integration.ipynb\n\nHow the memory footprint is reduced?\n\n\nWe log the `data_table` to W&B when the `on_train_begin` method is ivoked. Once it\u2019s uploaded as a W&B Artifact, we get a reference to this table which can be accessed using `data_table_ref` class variable. The `data_table_ref` is a 2D list that can be indexed like `self.data_table_ref[idx][n]` where `idx` is the row number while `n` is the column number. Let\u2019s see the usage in the example below.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"Although the answer is correct the retrieved document does not have any mention of the confusion matric documentation or why a table would be rendered instead of a confusion matrix chart as the user is experiencing. \n\nThe answer incorrectly suggests that recent API changes or visualization limitations could be factors not supported by any information in the document.\n\nFurthermore, it incorrectly proposes updating the library without evidence from the document that this step is necessary. The context does not support any of the troubleshooting steps in the answer."}
{"question":"How can I store data versions of large datasets using artefacts and tables? Such as millions of videos?","answer":"Storing large datasets, such as millions of videos, using Weights & Biases (wandb) artifacts and tables requires careful planning due to the size and complexity of the data. Here are some steps and considerations for managing large datasets with wandb:\n\n1. **Chunking the Data**: Instead of trying to upload the entire dataset at once, break it down into smaller, manageable chunks. For example, you could divide the videos into sets of thousands or tens of thousands, depending on their size.\n\n2. **Creating Artifacts for Chunks**: Create a separate artifact for each chunk of the dataset. This will help you manage versions and deduplication for each subset of the data.\n\n3. **Using References**: If the videos are already stored in a cloud storage solution (like AWS S3, Google Cloud Storage, or Azure Blob Storage), you can use wandb to create references to these files instead of uploading the actual video files. This way, you only store metadata and paths in wandb, which significantly reduces the storage requirements.\n\n4. **Version Control**: Use wandb's versioning system to keep track of changes to each chunk of the dataset. When you update a subset of the data, you can create a new version of the corresponding artifact.\n\n5. **Deduplication**: Wandb's deduplication feature will help save storage space by only storing the changes between versions of an artifact, rather than storing the entire artifact each time.\n\n6. **Metadata and Tables**: Instead of storing the video files in tables, store metadata about the videos in wandb tables. This metadata could include information like video file paths, labels, and any preprocessing information.\n\n7. **Asynchronous Uploading**: When dealing with large datasets, consider using asynchronous uploads to prevent your script from waiting for the upload to complete. This can be done by setting `async_=True` when logging artifacts.\n\n8. **Retention Policies**: Set retention or expiration policies on your artifacts to manage storage costs and comply with data privacy regulations. You can set a TTL (time-to-live) policy for each artifact.\n\nHere's an example of how you might reference large video files in an artifact without uploading the actual files:\n\n```python\nimport wandb\n\n# Initialize a new run\nrun = wandb.init(project=\"my_project\", job_type=\"log_dataset\")\n\n# Create a new artifact\nartifact = wandb.Artifact(\"video_dataset\", type=\"dataset\")\n\n# Assume we have a list of video file paths stored in cloud storage\nvideo_paths = [\"s3:\/\/mybucket\/video1.mp4\", \"s3:\/\/mybucket\/video2.mp4\", ...]\n\n# Add references to the artifact\nfor video_path in video_paths:\n    artifact.add_reference(video_path)\n\n# Log the artifact\nrun.log_artifact(artifact)\n\n# Finish the run\nrun.finish()\n```\n\nRemember that wandb tables are not designed to store actual large files like videos. They are more suitable for structured data, such as metadata or tabular data. For large files, you should use references to the storage location of the files and use wandb to track metadata and versions.","context":"Source:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/wandb-artifacts\/train_val_test_split_with_tabular_data.ipynb\n\nLoading the dataset into an artifact\n\n\nOur first step is to load in the dataset from a CSV file, which we accomplish with Artifacts and Tables. The wandb Artifact has two very useful features for our application: 1) it supports versioning, which will allow us to track changes we make to the original datset and 2) it supports deduplication, which will minimize the amount of storage space we use when generating modified versions of the dataset.\n\n\nIn the code below we use the python `requests` and `csv` libararies to load in each line of the CSV file into a wandb Table. Then we store the table in a wandb Artifact.\n\n\n\n```\n# Load raw dataset into a table and store it as an artifact\nwith wandb.init(project=PROJECT_NAME, job_type='load-data') as run:\n    # Load data row-by-row & add the rows to the table (Note: the whole table will be stored in memory)\n    dataset_url = 'https:\/\/s3-eu-west-1.amazonaws.com\/pstorage-leicester-213265548798\/23581310\/MyocardialinfarctioncomplicationsDatabase.csv'\n    with requests.get(dataset_url, stream=True) as r:\n        # Load each line into the table\n        lines = (line.decode('utf-8') for line in r.iter_lines())\n        column_headings = next(csv.reader(lines)) # This assumes that the first CSV line contains the column headings\n        data_table = wandb.Table(columns=column_headings) # Initialize the table\n        for index, row in enumerate(csv.reader(lines)): # Starting at the second row\n            row = data_types[entry_index if entry != '' else np.nan for entry_index, entry in enumerate(row)]\n            if len(row) == len(column_headings):\n                data_table.add_data(*row)\n    # Create an artifact for our dataset\n    dataset_artifact = wandb.Artifact(\n        'data-library', type='dataset',\n        description='Table containing the CSV dataset',\n        metadata={'MD5_checksum': 'd409a89bd7e566da4b82232c3956f576',\n                'filename': 'MyocardialinfarctioncomplicationsDatabase.csv',\n                'filesize': '427.31 kB',\n                'dataset_host': 'University of Leicester',\n                'dataset_url': dataset_url,\n                'project_url': 'https:\/\/doi.org\/10.25392\/leicester.data.12045261.v3',\n                'reference_doi': '10.25392\/leicester.data.12045261.v3',\n                }\n    )\n    # Add the table to the artifact & log the artifact\n    dataset_artifact.add(data_table, 'data-table')\n    run.log_artifact(dataset_artifact)\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/log\/log-tables\n\nAdvanced: Artifact Tables\n\n\nUse `artifact.add()` to log tables to the Artifacts section of your run instead of the workspace. This could be useful if you have a dataset that you want to log once and then reference for future runs. \n\n\n\n```\nrun = wandb.init(project=\"my_project\")\n# create a wandb Artifact for each meaningful step\ntest_predictions = wandb.Artifact(\"mnist_test_preds\", type=\"predictions\")\n\n# [build up your predictions data as above]\ntest_table = wandb.Table(data=data, columns=columns)\ntest_predictions.add(test_table, \"my_test_key\")\nrun.log_artifact(test_predictions)\n\n```\n\nRefer to this Colab for a detailed example of artifact.add() with image data and this Report for an example of how to use Artifacts and Tables to version control and deduplicate tabular data.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/examples\n\nArtifacts Examples\n\n\nWalk through an example of Artifacts in a live notebook.\n\n\n**Try the notebook \u2192**\n\n\n\nTo see how you can use Artifacts to version control tabular data, check out this report.\n\n\nFor more in-depth examples, try our two GitHub demo projects:\n\n\n\n2. **Detectron 2**: Image segmentation and bounding box prediction\u2014 visualize training metrics and predictions in real time, store and organize trained models, automatically track model fine-tuning, and save results\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/artifacts-faqs\n\nHow do I set a retention or expiration policy on my artifact?\n\n\nIf you have artifacts that are subject to data privacy regulations such as dataset artifacts containing PII, or want to schedule the deletion of an artifact version to manage your storage, you can set a TTL (time-to-live) policy. Learn more in this guide.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/tables\/tables-download\n\nConvert `table` to `artifact`\n\n\nFirst, you\u2019ll need to convert the table to an artifact. The easiest way to do this using `artifact.get(table, \"table_name\")`:\n\n\n\n```\n# Create and log a new table.\nwith wandb.init() as r:\n    artifact = wandb.Artifact(\"my_dataset\", type=\"dataset\")\n    table = wandb.Table(\n        columns=[\"a\", \"b\", \"c\"], data=[(i, i * 2, 2**i) for i in range(10)]\n    )\n    artifact.add(table, \"my_table\")\n    wandb.log_artifact(artifact)\n\n# Retrieve the created table using the artifact you created.\nwith wandb.init() as r:\n    artifact = r.use_artifact(\"my_dataset:latest\")\n    table = artifact.get(\"my_table\")\n\n```\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer provides a systematic approach for storing and managing large datasets, like videos, using Weights & Biases (wandb) artifacts and tables. The provided steps align with wandb's features like chunking data into manageable pieces, creating artifacts for chunks, using references to cloud storage, utilizing version control, employing deduplication, and setting retention policies. This efficient approach complies with wandb's capabilities for handling large volumes of data."}
{"question":"how to make runs in a sweep parallel","answer":"To make runs in a sweep parallel, you can launch multiple agents on different machines or within the same machine if it has multiple CPUs. Each agent will execute a run independently, allowing you to perform parallel experimentation. Here's how you can do it:\n\n### Parallelize on Multiple Machines\n\n1. **Set up your environment**: Ensure that the `wandb` library is installed on each machine you want to use for the sweep.\n\n2. **Launch agents**: On each machine, open a terminal and run the following command, replacing `sweep_id` with your actual sweep ID:\n\n   ```sh\n   wandb agent sweep_id\n   ```\n\n   You can launch up to 20 agents in parallel, depending on your plan with Weights & Biases.\n\n### Parallelize on a Multi-CPU Machine\n\n1. **Open multiple terminal windows**: On your local machine, open as many terminal windows as the number of CPUs you want to utilize.\n\n2. **Launch agents**: In each terminal window, run the `wandb agent` command with your sweep ID:\n\n   ```sh\n   wandb agent sweep_id\n   ```\n\n   This will start an agent in each terminal, allowing them to run in parallel on your multi-CPU machine.\n\n### Parallelize in Jupyter Notebooks\n\n1. **Open multiple Jupyter Notebooks**: Start as many Jupyter Notebooks as you want to run in parallel.\n\n2. **Launch agents**: In each Jupyter Notebook, execute the following code, replacing `sweep_id` with your actual sweep ID and `function_name` with the function that defines your training process:\n\n   ```python\n   import wandb\n   wandb.agent(sweep_id=sweep_id, function=function_name)\n   ```\n\n   This will start an agent in each Jupyter Notebook, allowing parallel execution of the sweep.\n\n### Tips for Parallelization\n\n- **Resource Management**: Ensure that each agent has enough resources (CPU, memory, GPU) to perform the training without causing resource contention.\n\n- **Environment Consistency**: Make sure that all environments where agents are running have the same dependencies and configurations to ensure consistent results.\n\n- **Monitoring**: Use the Weights & Biases dashboard to monitor the progress of each agent and compare results.\n\nBy following these steps, you can effectively parallelize runs in a sweep to speed up the hyperparameter search process.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project\n\n3. Launch agents\n\n\nNext, launch an agent locally. You can launch up to 20 agents on different machines in parallel if you want to distribute the work and finish the sweep job more quickly. The agent will print out the set of parameters it\u2019s trying next.\n\n\n\nNow you\u2019re running a sweep. The following image demonstrates what the dashboard looks like as the example sweep job is running. View an example project page \u2192\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/parallelize-agents\n\nParallelize on a multi-CPU machine\n\n\nDepending on your use case, explore the proceeding tabs to learn how to parallelize W&B Sweep agents using the CLI or within a Jupyter Notebook.\n\n\n\nUse the `wandb agent` command to parallelize your W&B Sweep agent across multiple CPUs with the terminal. Provide the sweep ID that was returned when you initialized the sweep. \n\n\n1. Open more than one terminal window on your local machine.\n2. Copy and paste the code snippet below and replace `sweep_id` with your sweep ID:\n\n\n\n```\nwandb agent sweep_id\n\n```\n\n\nUse the W&B Python SDK library to parallelize your W&B Sweep agent across multiple CPUs within Jupyter Notebooks. Ensure you have the sweep ID that was returned when you initialized the sweep. In addition, provide the name of the function the sweep will execute for the `function` parameter:\n\n\n1. Open more than one Jupyter Notebook.\n2. Copy and past the W&B Sweep ID on multiple Jupyter Notebooks to parallelize a W&B Sweep. For example, you can paste the following code snippet on multiple jupyter notebooks to paralleliz your sweep if you have the sweep ID stored in a variable called `sweep_id` and the name of the function is `function_name`:\n\n\n\n```\nwandb.agent(sweep_id=sweep_id, function=function_name)\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/boosting\/Using_W&B_Sweeps_with_XGBoost.ipynb\n\nDefine your training process\n\n\nBefore we can run the sweep,  \n\nwe need to define a function that creates and trains the model \u2013  \n\nthe function that takes in hyperparameter values and spits out metrics.\n\n\nWe\u2019ll also need `wandb` to be integrated into our script.  \n\nThere\u2019s three main components:  \n\n\\* `wandb.init()` \u2013 Initialize a new W&B run. Each run is single execution of the training script.  \n\n\\* `wandb.config` \u2013 Save all your hyperparameters in a config object. This lets you use our app to sort and compare your runs by hyperparameter values.  \n\n\\* `wandb.log()` \u2013 Logs metrics and custom objects \u2013 these can be images, videos, audio files, HTML, plots, point clouds etc.\n\n\nWe also need to download the data:\n\n\n\n```\n!wget https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/pima-indians-diabetes.data.csv\n\n```\n\n\n```\n# XGBoost model for Pima Indians dataset\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# load data\ndef train():\n  config_defaults = {\n    \"booster\": \"gbtree\",\n    \"max_depth\": 3,\n    \"learning_rate\": 0.1,\n    \"subsample\": 1,\n    \"seed\": 117,\n    \"test_size\": 0.33,\n  }\n\n  wandb.init(config=config_defaults)  # defaults are over-ridden during the sweep\n  config = wandb.config\n\n  # load data and split into predictors and targets\n  dataset = loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n  X, Y = dataset[:, :8], dataset[:, 8]\n\n  # split data into train and test sets\n  X_train, X_test, y_train, y_test = train_test_split(X, Y,\n                                                      test_size=config.test_size,\n                                                      random_state=config.seed)\n\n  # fit model on train\n  model = XGBClassifier(booster=config.booster, max_depth=config.max_depth,\n                        learning_rate=config.learning_rate, subsample=config.subsample)\n  model.fit(X_train, y_train)\n\n  # make predictions on test\n  y_pred = model.predict(X_test)\n  predictions = [round(value) for value in y_pred]\n\n  # evaluate predictions\n  accuracy = accuracy_score(y_test, predictions)\n  print(f\"Accuracy: {accuracy:.0%}\")\n  wandb.log({\"accuracy\": accuracy})\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project\n\n2. Create a sweep\n\n\nFrom your project page, open the Sweep tab in the sidebar and select **Create Sweep**.\n\n\n\nThe auto-generated configuration guesses values to sweep over based on the runs you have completed. Edit the configuration to specify what ranges of hyperparameters you want to try. When you launch the sweep, it starts a new process on the hosted W&B sweep server. This centralized service coordinates the agents\u2014 the machines that are running the training jobs.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project\n\nSeed a new sweep with existing runs\n\n\nLaunch a new sweep using existing runs that you\u2019ve previously logged.\n\n\n1. Open your project table.\n2. Select the runs you want to use with checkboxes on the left side of the table.\n3. Click the dropdown to create a new sweep.\n\n\nYour sweep will now be set up on our server. All you need to do is launch one or more agents to start running runs.\n\n\n\n:::info  \n\nIf you kick off the new sweep as a bayesian sweep, the selected runs will also seed the Gaussian Process.  \n\n:::\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer accurately explains how to run sweeps in parallel, both on multiple machines and on a multi-CPU machine, by launching multiple agents. It also provides an alternative approach for Jupyter Notebooks. This information is faithful to the recommendations given in the provided documentation\n"}
{"question":"Can I copy runs from one project from a team to another non-team project from? I can see all the runs in my wandb folder but I don't know how to sync those runs to another project.\nI need this because I have used all free tracked hours provided by weights and biases for the team entity and now I wish to transfer those runs to my personal project. There is an option for moving those runs as mentioned here (https:\/\/docs.wandb.ai\/guides\/runs\/manage-runs) using GUI, but now I can not access those runs as I am being redirected to billing page because of using all the tracked hours.","answer":"If you've used all the free tracked hours provided by Weights & Biases for your team entity and are unable to access the runs through the GUI because you're being redirected to the billing page, you may need to use the Weights & Biases API to programmatically move or copy the runs to your personal project.\n\nHere's a general approach you can take using the W&B API:\n\n1. **Install the W&B Python package** (if not already installed):\n   ```bash\n   pip install wandb\n   ```\n\n2. **Authenticate your W&B account**:\n   Use the `wandb login` command in your terminal and follow the instructions to log in to your account.\n\n3. **Write a script to copy runs**:\n   You can use the following script as a starting point to copy runs from your team project to your personal project. Make sure to replace `<team_entity>`, `<team_project>`, `<personal_entity>`, and `<personal_project>` with the appropriate values.\n\n   ```python\n   import wandb\n\n   # Authenticate with your W&B account\n   wandb.login()\n\n   # Set your team entity and project\n   team_entity = \"<team_entity>\"\n   team_project = \"<team_project>\"\n\n   # Set your personal entity and project\n   personal_entity = \"<personal_entity>\"\n   personal_project = \"<personal_project>\"\n\n   # Initialize the W&B API\n   api = wandb.Api()\n\n   # Get runs from the team project\n   team_runs = api.runs(path=f\"{team_entity}\/{team_project}\")\n\n   # Iterate over the runs and copy them to your personal project\n   for run in team_runs:\n       # You may need to adjust the following line to match the data you want to copy\n       # For example, you might want to copy the config, summary, and files\n       copied_run = api.create_run(f\"{personal_entity}\/{personal_project}\", run.name)\n       copied_run.config.update(run.config)\n       copied_run.summary.update(run.summary)\n       # You can also copy files associated with the run using run.files() and copied_run.upload_file()\n\n       # Mark the new run as finished\n       copied_run.finish()\n\n       print(f\"Copied run {run.name} to {personal_entity}\/{personal_project}\")\n   ```\n\nPlease note that this script is a simplified example and may require adjustments based on your specific needs, such as copying over files or handling large numbers of runs. Additionally, you should be aware of any API rate limits that may apply to your account.\n\nIf you encounter any issues or need further assistance, you may want to reach out to Weights & Biases support for help, especially since this involves a billing-related restriction. They may be able to provide temporary access or a solution tailored to your situation.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/app\/features\/runs-table\n\nMove runs between projects\n\n\nTo move runs from one project to another:\n\n\n1. Expand the table\n2. Click the checkbox next to the runs you want to move\n3. Click move and select the destination project\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/app\/pages\/workspaces\n\nTeam Projects\n\n\nEvery user of a team will get one workspace that is unique to them and can be customized to their liking. However, users between teams can switch workspaces to other users of the team. Workspaces can differ between users for a variety of reasons like having different custom charts, different filters\/groupings or section orders.\n\n\n\nAs such when viewing another team member\u2019s workspace, you can fork the workspace and then save it to your own by clicking on the Copy to My Workspace button.\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\n\nQuerying Multiple Runs\n\n\n\nThis example script finds a project and outputs a CSV of runs with name, configs and summary stats. Replace `<entity>` and `<project>` with your W&B entity and the name of your project, respectively.\n\n\n\n```\nimport pandas as pd\nimport wandb\n\napi = wandb.Api()\nentity, project = \"<entity>\", \"<project>\"\nruns = api.runs(entity + \"\/\" + project)\n\nsummary_list, config_list, name_list = [], [], []\nfor run in runs:\n    # .summary contains output keys\/values for\n    # metrics such as accuracy.\n    #  We call ._json_dict to omit large files\n    summary_list.append(run.summary._json_dict)\n\n    # .config contains the hyperparameters.\n    #  We remove special values that start with _.\n    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n\n    # .name is the human-readable name of the run.\n    name_list.append(run.name)\n\nruns_df = pd.DataFrame(\n    {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n)\n\nruns_df.to_csv(\"project.csv\")\n\n```\n\n\nThe W&B API also provides a way for you to query across runs in a project with api.runs(). The most common use case is exporting runs data for custom analysis. The query interface is the same as the one MongoDB uses.\n\n\n\n```\nruns = api.runs(\n    \"username\/project\",\n    {\"$or\": [{\"config.experiment_name\": \"foo\"}, {\"config.experiment_name\": \"bar\"}]},\n)\nprint(f\"Found {len(runs)} runs\")\n\n```\n\n\nCalling `api.runs` returns a `Runs` object that is iterable and acts like a list. By default the object loads 50 runs at a time in sequence as required, but you can change the number loaded per page with the `per_page` keyword argument.\n\n\n`api.runs` also accepts an `order` keyword argument. The default order is `-created_at`, specify `+created_at` to get results in ascending order. You can also sort by config or summary values e.g. `summary.val_acc` or `config.experiment_name`\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\n\nExport metrics from all runs in a project to a CSV file\n\n\nThis script pulls down the runs in a project and produces a dataframe and a CSV of runs including their names, configs, and summary stats. Replace `<entity>` and `<project>` with your W&B entity and the name of your project, respectively.\n\n\n\n```\nimport pandas as pd\nimport wandb\n\napi = wandb.Api()\nentity, project = \"<entity>\", \"<project>\"\nruns = api.runs(entity + \"\/\" + project)\n\nsummary_list, config_list, name_list = [], [], []\nfor run in runs:\n    # .summary contains the output keys\/values\n    #  for metrics such as accuracy.\n    #  We call ._json_dict to omit large files\n    summary_list.append(run.summary._json_dict)\n\n    # .config contains the hyperparameters.\n    #  We remove special values that start with _.\n    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n\n    # .name is the human-readable name of the run.\n    name_list.append(run.name)\n\nruns_df = pd.DataFrame(\n    {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n)\n\nruns_df.to_csv(\"project.csv\")\n\n```\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/reports\/cross-project-reports\n\nCross-project reports\n\n\n  \n\nCompare runs from two different projects.  \n\n\n\n\nCompare runs from two different projects with cross-project reports. Use the project selector in the run set table to pick a project.\n\n\n\nThe visualizations in the section pull columns from the first active runset. Make sure that the first run set checked in the section has that column available if you do not see the metric you are looking for in the line plot.\n\n\nThis feature supports history data on time series lines, but we don\u2019t support pulling different summary metrics from different projects. In other words, you can not create a scatter plot from columns that are only logged in another project.\n\n\nIf you need to compare runs from two projects and the columns are not working, add a tag to the runs in one project and then move those runs to the other project. You will still be able to filter only the runs from each project, but you will have all the columns for both sets of runs available in the report.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"While the provided answer does not fully address the user's issue of being unable to access the runs due to being redirected to the billing page, it does offer alternative methods, such as downloading the runs via API and re-uploading them to a personal project, which is the correct approach in this scenario."}
{"question":"I'm getting Internal Server Error for url: https:\/\/api.wandb.ai\/graphql when trying to download an artifact with the following code \n```\nprocessed_data_at = run.use_artifact(f'{params.PROCESSED_DATA_AT}:latest')\nprocessed_dataset_dir = Path(processed_data_at.download())\n```","answer":"An \"Internal Server Error\" typically indicates that there's a problem on the server side of the application you're trying to interact with\u2014in this case, the W&B (Weights & Biases) API. Here are some steps you can take to troubleshoot and potentially resolve the issue:\n\n1. **Check W&B Status**: Before diving into debugging, check if W&B is experiencing any known issues by visiting their status page. If there's a known outage or issue, it might be the cause of the error, and you'll have to wait until it's resolved.\n\n2. **Retry the Request**: Sometimes, intermittent server issues can cause this error. Try running your code again after a short wait to see if the issue resolves itself.\n\n3. **Review Your Code**: Double-check the artifact name and ensure that it's correctly formatted and exists in your W&B project. The `params.PROCESSED_DATA_AT` variable should contain the correct artifact name.\n\n4. **API Limits**: If you're making a lot of requests in a short period, you might be hitting rate limits. Review the W&B documentation to see if there are any rate limits that you might be exceeding.\n\n5. **Authentication**: Ensure that you're properly authenticated with W&B. If your authentication token is missing or incorrect, it could lead to errors. You can re-authenticate using the `wandb login` command.\n\n6. **W&B Version**: Check if you're using an outdated version of the W&B client library. If so, update to the latest version using `pip install --upgrade wandb`.\n\n7. **Error Logs**: Look for any additional error messages or logs that might provide more context about the error. Sometimes, the W&B client will output more detailed error information in the console.\n\n8. **Contact Support**: If none of the above steps work, reach out to W&B support with details of the error, including the full traceback and any relevant logs. They might be able to provide more specific guidance or information about the issue.\n\nHere's an example of how you might handle exceptions and retry the download in your code:\n\n```python\nfrom pathlib import Path\nimport wandb\nimport time\n\n# Initialize your W&B run\nrun = wandb.init()\n\n# Attempt to download the artifact with retries\nmax_retries = 3\nfor attempt in range(max_retries):\n    try:\n        processed_data_at = run.use_artifact(f'{params.PROCESSED_DATA_AT}:latest')\n        processed_dataset_dir = Path(processed_data_at.download())\n        break  # If the download is successful, break out of the loop\n    except Exception as e:\n        print(f\"Attempt {attempt+1} failed with error: {e}\")\n        if attempt < max_retries - 1:\n            time.sleep(2**attempt)  # Exponential backoff\n        else:\n            raise  # Re-raise the exception if all retries fail\n\n# Close the W&B run\nrun.finish()\n```\n\nRemember to replace `params.PROCESSED_DATA_AT` with the actual variable or string that contains your artifact name.","context":"Source:\thttps:\/\/docs.wandb.ai\/guides\/artifacts\/track-external-files\n\nDownload a reference artifact\n\n\n\n```\nimport wandb\n\nrun = wandb.init()\nartifact = run.use_artifact(\"mnist:latest\", type=\"dataset\")\nartifact_dir = artifact.download()\n\n```\n\nW&B will use the metadata recorded when the artifact was logged to retrieve the files from the underlying bucket when it downloads a reference artifact. If your bucket has object versioning enabled, W&B will retrieve the object version corresponding to the state of the file at the time an artifact was logged. This means that as you evolve the contents of your bucket, you can still point to the exact iteration of your data a given model was trained on since the artifact serves as a snapshot of your bucket at the time of training.\n\n\n:::info  \n\nW&B recommends that you enable \u2018Object Versioning\u2019 on your storage buckets if you overwrite files as part of your workflow. With versioning enabled on your buckets, artifacts with references to files that have been overwritten will still be intact because the older object versions are retained. \n\n\nBased on your use case, read the instructions to enable object versioning: AWS, GCP, Azure.  \n\n:::\n\n---\n\nSource:\thttps:\/\/docs.wandb.ai\/guides\/integrations\/add-wandb-to-any-library\n\nDownload A W&B Artifact\n\n\nYou re-use an Artifact (dataset, model\u2026) and `wandb` will download a copy locally (and cache it):\n\n\n\n```\nartifact = wandb.run.use_artifact(\u201cuser\/project\/artifact:latest\u201d)\nlocal_path = artifact.download(\u201c.\/tmp\u201d)\n\n```\n\nArtifacts can be found in the Artifacts section of W&B and can be referenced with aliases generated automatically (\u201clatest\u201d, \u201cv2\u201d, \u201cv3\u201d) or manually when logging (\u201cbest\\_accuracy\u201d\u2026).\n\n\nTo download an Artifact without creating a `wandb` run (through `wandb.init`), for example in distributed environments or for simple inference, you can instead reference the artifact with the wandb API:\n\n\n\n```\nartifact = wandb.Api().artifact(\u201cuser\/project\/artifact:latest\u201d)\nlocal_path = artifact.download()\n\n```\n\nFor more information, see Download and Use Artifacts.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/intro\/3_in_1_Intro_to_Weights_&_Biases_CV,_NLP_and_RL.ipynb\n\nDownload Data from W&B Artifacts\n\n\nWe use W&B Artifacts to download the super-mini iNaturalist 2021 dataset, a subset of the iNaturalist 2021 dataset with only 50 classes with 50 examples each.\n\n\nW&B Artifacts is a versioning system for  \n\nmodels, datasets, and other large files.  \n\nRead more in our docs here.\n\n\n\n```\n# \u270d\ufe0f Start a wandb run and add any additional info you'd like to the config  \nWANDB_CV_PROJECT = \"iNat2021\"\nrun = wandb.init(project=WANDB_CV_PROJECT)  # config is optional here\n\n# \u270d\ufe0f Identify the W&B Artifact where the dataset is stored and download it\nartifact = run.use_artifact(\"wandb\/iNat2021\/data_supermini_iNat2021:v1\", type=\"dataset\")\ndata_dir = artifact.download()\n\n# \u270d\ufe0f Close your W&B Run\nrun.finish()\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/keras\/Zero_DCE_Colab_Keras(video).ipynb\n\nDownloading Lol dataset using W&B Artifacts\n\n\n\n```\nimport os\nimport random\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n```\n\n\n```\nrun = wandb.init()\nartifact = run.use_artifact('ivangoncharov\/Low Light Enhancement with Zero-DCE\/Lol_Dataset:v0', type='dataset')\nartifact_dir = artifact.download()\nartifact_path = os.path.join(artifact_dir, \"lol_dataset.zip\")\n!unzip $artifact_path\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/wandb-artifacts\/Pipeline_Versioning_with_W&B_Artifacts.ipynb\n\n\ud83c\udf10 The Artifacts page on wandb.ai\n\n\nNow that we\u2019ve logged and used an `Artifact`,  \n\nlet\u2019s check out the Artifacts tab on the Run page.\n\n\nNavigate to the Run page URL from the `wandb` output  \n\nand select the \u201cArtifacts\u201d tab from the left sidebar  \n\n(it\u2019s the one with the database icon,  \n\nwhich looks like three hockey pucks stacked on top of one another).\n\n\nClick a row in either the \u201cInput Artifacts\u201d table  \n\nor in the \u201cOutput Artifacts\u201d table,  \n\nthen check out the tabs (\u201cOverview\u201d, \u201cMetadata\u201d)  \n\nto see everything logged about the `Artifact`.\n\n\nWe particularly like the \u201cGraph View\u201d.  \n\nBy default, it shows a graph  \n\nwith the `type`s of `Artifact`s  \n\nand the `job_type`s of `Run` as the two types of nodes,  \n\nwith arrows to represent consumption and production.\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The suggested steps are relevant and provide a comprehensive approach to troubleshooting the issue, including checking system status, re-attempting the request, validating code correctness, considering API limits, verifying authentication, updating the client library, reviewing error logs, and contacting support. The provided code snippet appropriately demonstrates how to implement retry logic with exponential backoff in response to exceptions."}
{"question":"do you support yolov5? how can i log COCO bounding boxes?","answer":"Yes, YOLOv5 is designed to work with Weights & Biases (W&B) for logging and visualizing training runs, including COCO bounding boxes. To log COCO bounding boxes during your YOLOv5 training sessions, you need to follow these steps:\n\n1. **Install Weights & Biases**: If you haven't already installed W&B, you can do so using pip. Here's the command to install a specific version of W&B:\n\n    ```python\n    %%capture\n    !pip install \"wandb==0.12.10\"\n    ```\n\n2. **Set Up Your W&B Account**: If it's your first time using W&B, you'll be prompted to set up an account. Follow the instructions to sign up and log in.\n\n3. **Configure YOLOv5 Training with W&B Logging**: When you run the `train.py` script for YOLOv5, you can include additional command-line arguments to enable W&B logging and configure how your bounding boxes and other data are logged. Here's an example command that includes arguments for W&B:\n\n    ```python\n    !python train.py \\\n      --img 640 \\\n      --batch 64 \\\n      --epochs 5 \\\n      --data coco128.yaml \\\n      --weights yolov5s.pt \\\n      --project your_project_name \\\n      --upload_dataset \\\n      --bbox_interval 1 \\\n      --save-period 1\n    ```\n\n    - `--project`: Sets the W&B project name where your training run will be logged.\n    - `--upload_dataset`: Uploads the dataset to W&B for visualization.\n    - `--bbox_interval`: Determines how often (in epochs) to log bounding box predictions on the validation set.\n    - `--save-period`: Sets the interval (in epochs) at which model checkpoints are saved and logged to W&B.\n\n4. **Run the Training**: Execute the training command. During training, W&B will log metrics, system stats, and bounding box predictions to your W&B dashboard.\n\n5. **Visualize Results**: After starting the training, you'll get a link to your W&B dashboard where you can monitor the training progress in real-time. Look for the \"Images\" panel in the \"Media\" section to see the logged bounding box visualizations.\n\n6. **Share Results**: You can create detailed reports of your results in W&B and share them with your team or the community.\n\nRemember to replace `your_project_name` with the actual name of your W&B project, and adjust other command-line arguments as needed for your specific training setup.","context":"Source:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/yolo\/Logging_YOLOv5_Experiments_with_W&B.ipynb\n\nWeights & Biases Logging (\ud83d\ude80 NEW)\n\n\nWeights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install `wandb`, and then train normally (you will be guided through setting up `wandb` account during your first use).\n\n\n\n```\n%%capture\n!pip install \"wandb==0.12.10\"\n\n```\n\nAnd that\u2019s it! So long as W&B is installed, you\u2019ll get rich, detailed metrics in a live dashboard accessible from a browser on any device.\n\n\nJust click the link that appears below next to `wandb` and the \ud83d\ude80 emoji.\n\n\n\n```\n# Train YOLOv5s on COCO128 for 5 epochs\n!python train.py --img 640 --batch 64 --epochs 5 --data coco128.yaml --weights yolov5s.pt\n\n```\n\nWith W&B, during training you will see live updates on the dashboard at wandb.ai, including interactive bounding box visualizations (look for a panel called \u201cImages\u201d in the Media panel section), and you can create and share detailed Reports of your results. For more information see the YOLOv5 Weights & Biases Tutorial  \n\nor check out the video tutorial for this notebook.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/yolo\/Train_YOLOv5_model_on_a_Custom_Dataset_with_Weights_&_Biases.ipynb\n\nTrain\n\n\nYour custom classes (probably) are not among the objects in COCO,  \n\nso our pretrained models don\u2019t know how to detect them  \n\nand we can\u2019t just use `detect.py` with one of those models.\n\n\nInstead, we need to train the models to detect our custom classes,  \n\nusing YOLOv5\u2019s `train.py`.  \n\nWe don\u2019t have to start our models from scratch though!  \n\nWe can finetune the pretrained models on our custom dataset.  \n\nThis substantially speeds up training.\n\n\nModel training is a complex process,  \n\nso we\u2019ll want to track the inputs and outputs,  \n\nlog information about model behavior during training,  \n\nand record system state and metrics.\n\n\nThat\u2019s where Weights & Biases  \n\ncomes in:  \n\nthe `wandb` library provides all the tools you need to thoroughly  \n\nand effectively log model training experiments.\n\n\nYOLOv5 comes with `wandb` already integrated,  \n\nso all you need to do is configure the logging  \n\nwith command line arguments.\n\n\n* `--project` sets the W&B project to which we\u2019re logging  \n\n(akin to a GitHub repo).\n* `--upload_dataset` tells `wandb`  \n\nto upload the dataset as a dataset-visualization Table.  \n\nAt regular intervals set by `--bbox_interval`,  \n\nthe model\u2019s outputs on the validation set will also be logged to W&B.\n* `--save_period` sets the number of epochs to wait  \n\nin between logging the model checkpoints.  \n\nIf not set, only the final trained model is logged.\n\n\nEven without these arguments,  \n\nbasic model metrics and some model outputs will still be saved to W&B.\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/yolo\/Wildfire_Smoke_Detection_with_YOLOv5,_Roboflow_and_Weights_&_Biases (1).ipynb\n\nTrain\n\n\nYOLOv5 comes with `wandb` already integrated,  \n\nso all you need to do is configure the logging  \n\nwith command line arguments.\n\n\n* `--project` sets the W&B project to which we\u2019re logging  \n\n(akin to a GitHub repo).\n* `--upload_dataset` tells `wandb`  \n\nto upload the dataset as a dataset-visualization Table.  \n\nAt regular intervals set by `--bbox_interval`,  \n\nthe model\u2019s outputs on the validation set will also be logged to W&B.\n* `--save-period` sets the number of epochs to wait  \n\nin between logging the model checkpoints.  \n\nIf not set, only the final trained model is logged.\n\n\nEven without these arguments,  \n\nbasic model metrics and some model outputs will still be saved to W&B.\n\n\n\n> \n> *Note*: to use this same training and logging setup on a different dataset,  \n> \n> just create a `data.yaml` for that dataset  \n> \n> and provide it to the `--data` argument.\n> \n> \n> \n\n\n\n```\n!python train.py \\\n  --data ..\/data.yaml \\\n  --epochs 10 \\\n  --project yolov5-roboflow-wandb \\\n  --upload_dataset \\\n  --bbox_interval 1 \\\n  --save-period 1\n\n```\n\nHere\u2019s where you can find the uploaded evaluation results in the W&B UI:\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/yolo\/Train_YOLOv5_model_on_a_Custom_Dataset_with_Weights_&_Biases.ipynb\n\nDetect\n\n\nYOLOv5 provides highly-accurate, fast models that are pretrained on the  \n\nCommon Objects in COntext (COCO) dataset.\n\n\nIf your object detection application involves only  \n\nclasses from the COCO dataset,  \n\nlike \u201cStop Sign\u201d and \u201cPizza\u201d,  \n\nthen these pretrained models may be all you need!\n\n\nThe cell below runs a pretrained model on an example image  \n\nusing `detect.py` from the YOLOv5 toolkit.\n\n\n\n```\nfrom IPython.display import Image\n\n!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data\/images\/bus.jpg\nImage(filename='runs\/detect\/exp\/bus.jpg', width=600)\n\n```\n\n---\n\nSource:\thttps:\/\/github.com\/wandb\/examples\/tree\/master\/colabs\/paddlepaddle\/paddledetection\/PaddleDetection_and_W&B_Your_one_stop_for_everything_object_detection.ipynb\n\nTesting and Logging annotated images to your W&B dashboard \ud83d\udd25\n\n\nThe following cell runs the inference script on all the images in `demo` directory and stores the annotated images with the bounding boxes in the `infer_output` directory using the YOLOX model pulled from W&B.\n\n\n\n```\n%%shell\nfor i in $(ls demo\/*.jpg)\ndo\n    python tools\/infer.py -c configs\/yolox\/yolox_nano_300e_coco.yml \\\n                    --infer_img=$i \\\n                    --output_dir=infer_output\/ \\\n                    --draw_threshold=0.5 \\\n                    -o weights=.\/artifacts\/model-26oqc38r:v1\/model\ndone\n\n```\n\nWith the images being annotated, we will now initialize a new run in the project and a W&B table which we will then use to add the input and output images side-by-side.\n\n\n\n```\nwandb.init(project=\"PaddleDetectionYOLOX\")\nwandb.use_artifact('manan-goel\/PaddleDetectionYOLOX\/model-26oqc38r:best')\ntable = wandb.Table(columns=[\"Input Image\", \"Annotated Image\"])\n\ninp_imgs = sorted(glob.glob(\".\/demo\/*.jpg\"), key=lambda x: x.split(\"\/\")[-1])\nout_imgs = sorted(glob.glob(\".\/infer_output\/*.jpg\"), key=lambda x: x.split(\"\/\")[-1])\n\n```\n\nThe next cell iterates over the images and adds the corresponding inputs and outputs as a row in the table initialized in the previous cell.\n\n\n\n```\nfor inp in inp_imgs:\n    for out in out_imgs:\n        if out.split(\"\/\")[-1] != inp.split(\"\/\")[-1]:\n            continue\n        table.add_data(\n            wandb.Image(inp),\n            wandb.Image(out)\n        )\nwandb.log({\n    \"Predictions\": table\n})\nwandb.finish()\n\n```\n\nThis provides a really cool debugging tool!\n\n---\n\n","correctness":"correct","is_wandb_query":"YES","notes":"The answer provides the correct advice for integrating Weights & Biases with YOLOv5 for logging COCO bounding boxes and provides accurate information based on the provided documentation."}
